{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devubavaria/Water-Quality-Analysis/blob/main/MP_FINAL_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJHqnsCFdmhf",
        "outputId": "ff07dab6-c46e-4e3d-f52f-3494a9926df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/water_potability.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "u-uFFpiedrfH",
        "outputId": "a208c527-0a70-41a1-836d-6625738d172a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            ph    Hardness        Solids  Chloramines     Sulfate  \\\n",
              "0          NaN  204.890455  20791.318981     7.300212  368.516441   \n",
              "1     3.716080  129.422921  18630.057858     6.635246         NaN   \n",
              "2     8.099124  224.236259  19909.541732     9.275884         NaN   \n",
              "3     8.316766  214.373394  22018.417441     8.059332  356.886136   \n",
              "4     9.092223  181.101509  17978.986339     6.546600  310.135738   \n",
              "...        ...         ...           ...          ...         ...   \n",
              "3271  4.668102  193.681735  47580.991603     7.166639  359.948574   \n",
              "3272  7.808856  193.553212  17329.802160     8.061362         NaN   \n",
              "3273  9.419510  175.762646  33155.578218     7.350233         NaN   \n",
              "3274  5.126763  230.603758  11983.869376     6.303357         NaN   \n",
              "3275  7.874671  195.102299  17404.177061     7.509306         NaN   \n",
              "\n",
              "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
              "0       564.308654       10.379783        86.990970   2.963135           0  \n",
              "1       592.885359       15.180013        56.329076   4.500656           0  \n",
              "2       418.606213       16.868637        66.420093   3.055934           0  \n",
              "3       363.266516       18.436524       100.341674   4.628771           0  \n",
              "4       398.410813       11.558279        31.997993   4.075075           0  \n",
              "...            ...             ...              ...        ...         ...  \n",
              "3271    526.424171       13.894419        66.687695   4.435821           1  \n",
              "3272    392.449580       19.903225              NaN   2.798243           1  \n",
              "3273    432.044783       11.039070        69.845400   3.298875           1  \n",
              "3274    402.883113       11.168946        77.488213   4.708658           1  \n",
              "3275    327.459760       16.140368        78.698446   2.309149           1  \n",
              "\n",
              "[3276 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-735d78e9-1889-42eb-a075-06737c5b0a94\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ph</th>\n",
              "      <th>Hardness</th>\n",
              "      <th>Solids</th>\n",
              "      <th>Chloramines</th>\n",
              "      <th>Sulfate</th>\n",
              "      <th>Conductivity</th>\n",
              "      <th>Organic_carbon</th>\n",
              "      <th>Trihalomethanes</th>\n",
              "      <th>Turbidity</th>\n",
              "      <th>Potability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>204.890455</td>\n",
              "      <td>20791.318981</td>\n",
              "      <td>7.300212</td>\n",
              "      <td>368.516441</td>\n",
              "      <td>564.308654</td>\n",
              "      <td>10.379783</td>\n",
              "      <td>86.990970</td>\n",
              "      <td>2.963135</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.716080</td>\n",
              "      <td>129.422921</td>\n",
              "      <td>18630.057858</td>\n",
              "      <td>6.635246</td>\n",
              "      <td>NaN</td>\n",
              "      <td>592.885359</td>\n",
              "      <td>15.180013</td>\n",
              "      <td>56.329076</td>\n",
              "      <td>4.500656</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.099124</td>\n",
              "      <td>224.236259</td>\n",
              "      <td>19909.541732</td>\n",
              "      <td>9.275884</td>\n",
              "      <td>NaN</td>\n",
              "      <td>418.606213</td>\n",
              "      <td>16.868637</td>\n",
              "      <td>66.420093</td>\n",
              "      <td>3.055934</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.316766</td>\n",
              "      <td>214.373394</td>\n",
              "      <td>22018.417441</td>\n",
              "      <td>8.059332</td>\n",
              "      <td>356.886136</td>\n",
              "      <td>363.266516</td>\n",
              "      <td>18.436524</td>\n",
              "      <td>100.341674</td>\n",
              "      <td>4.628771</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9.092223</td>\n",
              "      <td>181.101509</td>\n",
              "      <td>17978.986339</td>\n",
              "      <td>6.546600</td>\n",
              "      <td>310.135738</td>\n",
              "      <td>398.410813</td>\n",
              "      <td>11.558279</td>\n",
              "      <td>31.997993</td>\n",
              "      <td>4.075075</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3271</th>\n",
              "      <td>4.668102</td>\n",
              "      <td>193.681735</td>\n",
              "      <td>47580.991603</td>\n",
              "      <td>7.166639</td>\n",
              "      <td>359.948574</td>\n",
              "      <td>526.424171</td>\n",
              "      <td>13.894419</td>\n",
              "      <td>66.687695</td>\n",
              "      <td>4.435821</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3272</th>\n",
              "      <td>7.808856</td>\n",
              "      <td>193.553212</td>\n",
              "      <td>17329.802160</td>\n",
              "      <td>8.061362</td>\n",
              "      <td>NaN</td>\n",
              "      <td>392.449580</td>\n",
              "      <td>19.903225</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.798243</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3273</th>\n",
              "      <td>9.419510</td>\n",
              "      <td>175.762646</td>\n",
              "      <td>33155.578218</td>\n",
              "      <td>7.350233</td>\n",
              "      <td>NaN</td>\n",
              "      <td>432.044783</td>\n",
              "      <td>11.039070</td>\n",
              "      <td>69.845400</td>\n",
              "      <td>3.298875</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3274</th>\n",
              "      <td>5.126763</td>\n",
              "      <td>230.603758</td>\n",
              "      <td>11983.869376</td>\n",
              "      <td>6.303357</td>\n",
              "      <td>NaN</td>\n",
              "      <td>402.883113</td>\n",
              "      <td>11.168946</td>\n",
              "      <td>77.488213</td>\n",
              "      <td>4.708658</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3275</th>\n",
              "      <td>7.874671</td>\n",
              "      <td>195.102299</td>\n",
              "      <td>17404.177061</td>\n",
              "      <td>7.509306</td>\n",
              "      <td>NaN</td>\n",
              "      <td>327.459760</td>\n",
              "      <td>16.140368</td>\n",
              "      <td>78.698446</td>\n",
              "      <td>2.309149</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3276 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-735d78e9-1889-42eb-a075-06737c5b0a94')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-735d78e9-1889-42eb-a075-06737c5b0a94 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-735d78e9-1889-42eb-a075-06737c5b0a94');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-40935f84-f1a3-4465-ac6c-3d3f4fd91a7c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40935f84-f1a3-4465-ac6c-3d3f4fd91a7c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-40935f84-f1a3-4465-ac6c-3d3f4fd91a7c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5VIe5hCdydD"
      },
      "outputs": [],
      "source": [
        "df['ph'].fillna(value=df['ph'].median(),inplace=True)\n",
        "df['Sulfate'].fillna(value=df['Sulfate'].median(),inplace=True)\n",
        "df['Trihalomethanes'].fillna(value=df['Trihalomethanes'].median(),inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgTiI8qYd-w0",
        "outputId": "f16a13e6-e30a-4bfb-b2b7-96fab2df4226"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ph                 0\n",
              "Hardness           0\n",
              "Solids             0\n",
              "Chloramines        0\n",
              "Sulfate            0\n",
              "Conductivity       0\n",
              "Organic_carbon     0\n",
              "Trihalomethanes    0\n",
              "Turbidity          0\n",
              "Potability         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8ms445VeD3i"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['Potability'], axis='columns')\n",
        "y = df.Potability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cqAB3syeGxR"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "\n",
        "# Assuming you have your dataset in X and y\n",
        "\n",
        "# SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# ADASYN\n",
        "adasyn = ADASYN(sampling_strategy='auto', random_state=42)\n",
        "X_adasyn, y_adasyn = adasyn.fit_resample(X, y)\n",
        "\n",
        "# Borderline-SMOTE\n",
        "borderline_smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_borderline, y_borderline = borderline_smote.fit_resample(X, y)\n",
        "\n",
        "# SVMSMOTE\n",
        "svm_smote = SVMSMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_svm_smote, y_svm_smote = svm_smote.fit_resample(X, y)\n",
        "\n",
        "# SMOTE-Tomek\n",
        "smote_tomek = SMOTETomek(sampling_strategy='auto', random_state=42)\n",
        "X_smote_tomek, y_smote_tomek = smote_tomek.fit_resample(X, y)\n",
        "\n",
        "# SMOTE-ENN\n",
        "smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)\n",
        "X_smote_enn, y_smote_enn = smote_enn.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPILJHG0eJgF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "features_scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "features_smote = features_scaler.fit_transform(X_smote)\n",
        "features_adasyn = features_scaler.fit_transform(X_adasyn)\n",
        "features_borderline = features_scaler.fit_transform(X_borderline)\n",
        "features_svm_smote = features_scaler.fit_transform(X_svm_smote)\n",
        "features_smote_tomek= features_scaler.fit_transform(X_smote_tomek)\n",
        "features_smote_enn= features_scaler.fit_transform(X_smote_enn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4Urth5XeMxI"
      },
      "outputs": [],
      "source": [
        "features_simple= features_scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D7BEJVTCeSGs",
        "outputId": "fa1de3a9-30ed-4684-a37e-1768fcf9ec2a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 503, number of negative: 382\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 885, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.568362 -> initscore=0.275170\n",
            "[LightGBM] [Info] Start training from score 0.275170\n",
            "[LightGBM] [Info] Number of positive: 503, number of negative: 382\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 885, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.568362 -> initscore=0.275170\n",
            "[LightGBM] [Info] Start training from score 0.275170\n",
            "[LightGBM] [Info] Number of positive: 504, number of negative: 382\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.568849 -> initscore=0.277156\n",
            "[LightGBM] [Info] Start training from score 0.277156\n",
            "[LightGBM] [Info] Number of positive: 503, number of negative: 383\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.567720 -> initscore=0.272555\n",
            "[LightGBM] [Info] Start training from score 0.272555\n",
            "[LightGBM] [Info] Number of positive: 503, number of negative: 383\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 886, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.567720 -> initscore=0.272555\n",
            "[LightGBM] [Info] Start training from score 0.272555\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_dfe40_row0_col1 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_dfe40_row1_col1 {\n",
              "  background-color: #f5eff6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_dfe40_row2_col1 {\n",
              "  background-color: #f4eef6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_dfe40_row3_col1 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_dfe40_row4_col1 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_dfe40_row5_col1 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_dfe40_row6_col1 {\n",
              "  background-color: #5c9fc9;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_dfe40_row7_col1 {\n",
              "  background-color: #509ac6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_dfe40_row8_col1 {\n",
              "  background-color: #1e80b8;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_dfe40_row9_col1 {\n",
              "  background-color: #197db7;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_dfe40_row10_col1 {\n",
              "  background-color: #023858;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_dfe40\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_dfe40_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_dfe40_level0_col1\" class=\"col_heading level0 col1\" >cv_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row0\" class=\"row_heading level0 row0\" >9</th>\n",
              "      <td id=\"T_dfe40_row0_col0\" class=\"data row0 col0\" >MLPClassifier()</td>\n",
              "      <td id=\"T_dfe40_row0_col1\" class=\"data row0 col1\" >0.550972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row1\" class=\"row_heading level0 row1\" >6</th>\n",
              "      <td id=\"T_dfe40_row1_col0\" class=\"data row1 col0\" >SVC()</td>\n",
              "      <td id=\"T_dfe40_row1_col1\" class=\"data row1 col1\" >0.568203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row2\" class=\"row_heading level0 row2\" >8</th>\n",
              "      <td id=\"T_dfe40_row2_col0\" class=\"data row2 col0\" >LogisticRegression()</td>\n",
              "      <td id=\"T_dfe40_row2_col1\" class=\"data row2 col1\" >0.570923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
              "      <td id=\"T_dfe40_row3_col0\" class=\"data row3 col0\" >ExtraTreeClassifier()</td>\n",
              "      <td id=\"T_dfe40_row3_col1\" class=\"data row3 col1\" >0.639570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row4\" class=\"row_heading level0 row4\" >0</th>\n",
              "      <td id=\"T_dfe40_row4_col0\" class=\"data row4 col0\" >AdaBoostClassifier()</td>\n",
              "      <td id=\"T_dfe40_row4_col1\" class=\"data row4 col1\" >0.652244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row5\" class=\"row_heading level0 row5\" >3</th>\n",
              "      <td id=\"T_dfe40_row5_col0\" class=\"data row5 col0\" >DecisionTreeClassifier()</td>\n",
              "      <td id=\"T_dfe40_row5_col1\" class=\"data row5 col1\" >0.672076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row6\" class=\"row_heading level0 row6\" >2</th>\n",
              "      <td id=\"T_dfe40_row6_col0\" class=\"data row6 col0\" >GradientBoostingClassifier()</td>\n",
              "      <td id=\"T_dfe40_row6_col1\" class=\"data row6 col1\" >0.699266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row7\" class=\"row_heading level0 row7\" >1</th>\n",
              "      <td id=\"T_dfe40_row7_col0\" class=\"data row7 col0\" >BaggingClassifier()</td>\n",
              "      <td id=\"T_dfe40_row7_col1\" class=\"data row7 col1\" >0.705560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row8\" class=\"row_heading level0 row8\" >7</th>\n",
              "      <td id=\"T_dfe40_row8_col0\" class=\"data row8 col0\" >RandomForestClassifier()</td>\n",
              "      <td id=\"T_dfe40_row8_col1\" class=\"data row8 col1\" >0.736248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row9\" class=\"row_heading level0 row9\" >10</th>\n",
              "      <td id=\"T_dfe40_row9_col0\" class=\"data row9 col0\" >LGBMClassifier()</td>\n",
              "      <td id=\"T_dfe40_row9_col1\" class=\"data row9 col1\" >0.738979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_dfe40_level0_row10\" class=\"row_heading level0 row10\" >5</th>\n",
              "      <td id=\"T_dfe40_row10_col0\" class=\"data row10 col0\" >KNeighborsClassifier()</td>\n",
              "      <td id=\"T_dfe40_row10_col1\" class=\"data row10 col1\" >0.821137</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7e7a35eda020>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from yellowbrick.classifier import ROCAUC\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "mod = []\n",
        "cv_score=[]\n",
        "model =[AdaBoostClassifier(), BaggingClassifier(), GradientBoostingClassifier(), DecisionTreeClassifier(),\n",
        "        ExtraTreeClassifier(), KNeighborsClassifier(), SVC(), RandomForestClassifier(), LogisticRegression(),\n",
        "        MLPClassifier(), LGBMClassifier() ]\n",
        "for m in model:\n",
        "    cv_score.append(cross_val_score(m, X_smote_enn, y_smote_enn , scoring='accuracy', cv=5).mean())\n",
        "    mod.append(m)\n",
        "model_df=pd.DataFrame(columns=['model','cv_score'])\n",
        "model_df['model']=mod\n",
        "model_df['cv_score']=cv_score\n",
        "model_df.sort_values(by=['cv_score'], ascending=True).style.background_gradient(subset=['cv_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpQMfdKeoi6W"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_params = {\n",
        "    'svm': {\n",
        "        'model': SVC(gamma='auto'),\n",
        "        'params' : {\n",
        "            'C': [1,10,20,30,50],\n",
        "            'kernel': ['rbf','linear','poly']\n",
        "        }\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params' : {\n",
        "            'n_estimators': [60,70,80,100,200,300,400,500,600,700]\n",
        "        }\n",
        "    },\n",
        "    'logistic_regression' : {\n",
        "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
        "        'params': {\n",
        "            'C': [1,5,10,15,20,25,30]\n",
        "        }\n",
        "    },\n",
        "    'KNN' : {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'n_neighbors': [1,3,5,7,10,12,16,17,20,21,25,30,35,40,42,44,48,50,55,59,60]\n",
        "        }\n",
        "    },\n",
        "    'Gradient_Boosting':\n",
        "        {\n",
        "            'model': GradientBoostingClassifier(),\n",
        "            'params' : {\n",
        "            'n_estimators': [60,70,80,100,200,300,400,500,600,700]\n",
        "        }\n",
        "        },\n",
        "    'Bagging_Classifier':\n",
        "    {\n",
        "        'model': BaggingClassifier(),\n",
        "        'params' : {\n",
        "         'n_estimators': [60,70,80,100,200,300,400,500,600,700]\n",
        "        }\n",
        "    },\n",
        "    'DecisionTree_Classifier':\n",
        "    {\n",
        "        'model': DecisionTreeClassifier(),\n",
        "        'params' : { 'criterion': ['gini']\n",
        "        }\n",
        "    }\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "4rQj0z3Yok7D",
        "outputId": "41dcece0-b818-48af-f23e-5a7f52f71aa9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_84c06_row0_col1 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_84c06_row1_col1 {\n",
              "  background-color: #ebe6f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_84c06_row2_col1 {\n",
              "  background-color: #4e9ac6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_84c06_row3_col1 {\n",
              "  background-color: #046097;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_84c06_row4_col1 {\n",
              "  background-color: #03466e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_84c06_row5_col1 {\n",
              "  background-color: #023c5f;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_84c06_row6_col1 {\n",
              "  background-color: #023858;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_84c06\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_84c06_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_84c06_level0_col1\" class=\"col_heading level0 col1\" >best_score</th>\n",
              "      <th id=\"T_84c06_level0_col2\" class=\"col_heading level0 col2\" >best_params</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_84c06_level0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
              "      <td id=\"T_84c06_row0_col0\" class=\"data row0 col0\" >logistic_regression</td>\n",
              "      <td id=\"T_84c06_row0_col1\" class=\"data row0 col1\" >0.577200</td>\n",
              "      <td id=\"T_84c06_row0_col2\" class=\"data row0 col2\" >{'C': 1}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_84c06_level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
              "      <td id=\"T_84c06_row1_col0\" class=\"data row1 col0\" >svm</td>\n",
              "      <td id=\"T_84c06_row1_col1\" class=\"data row1 col1\" >0.599812</td>\n",
              "      <td id=\"T_84c06_row1_col2\" class=\"data row1 col2\" >{'C': 50, 'kernel': 'rbf'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_84c06_level0_row2\" class=\"row_heading level0 row2\" >6</th>\n",
              "      <td id=\"T_84c06_row2_col0\" class=\"data row2 col0\" >DecisionTree_Classifier</td>\n",
              "      <td id=\"T_84c06_row2_col1\" class=\"data row2 col1\" >0.676597</td>\n",
              "      <td id=\"T_84c06_row2_col2\" class=\"data row2 col2\" >{'criterion': 'gini'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_84c06_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_84c06_row3_col0\" class=\"data row3 col0\" >KNN</td>\n",
              "      <td id=\"T_84c06_row3_col1\" class=\"data row3 col1\" >0.721805</td>\n",
              "      <td id=\"T_84c06_row3_col2\" class=\"data row3 col2\" >{'n_neighbors': 1}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_84c06_level0_row4\" class=\"row_heading level0 row4\" >1</th>\n",
              "      <td id=\"T_84c06_row4_col0\" class=\"data row4 col0\" >random_forest</td>\n",
              "      <td id=\"T_84c06_row4_col1\" class=\"data row4 col1\" >0.740744</td>\n",
              "      <td id=\"T_84c06_row4_col2\" class=\"data row4 col2\" >{'n_estimators': 80}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_84c06_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_84c06_row5_col0\" class=\"data row5 col0\" >Bagging_Classifier</td>\n",
              "      <td id=\"T_84c06_row5_col1\" class=\"data row5 col1\" >0.747096</td>\n",
              "      <td id=\"T_84c06_row5_col2\" class=\"data row5 col2\" >{'n_estimators': 70}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_84c06_level0_row6\" class=\"row_heading level0 row6\" >4</th>\n",
              "      <td id=\"T_84c06_row6_col0\" class=\"data row6 col0\" >Gradient_Boosting</td>\n",
              "      <td id=\"T_84c06_row6_col1\" class=\"data row6 col1\" >0.749847</td>\n",
              "      <td id=\"T_84c06_row6_col2\" class=\"data row6 col2\" >{'n_estimators': 700}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7e7a34d63730>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "scores = []\n",
        "\n",
        "for model_name, mp in model_params.items():\n",
        "    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
        "    clf.fit(features_smote_enn, y_smote_enn)\n",
        "    scores.append({\n",
        "        'model': model_name,\n",
        "        'best_score': clf.best_score_,\n",
        "        'best_params': clf.best_params_\n",
        "    })\n",
        "\n",
        "df_score = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
        "df_score.sort_values(by=['best_score'], ascending=True).style.background_gradient(subset=['best_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BupiwlUSVFIr"
      },
      "source": [
        "<H3> ALL THE ABOVE MODELS ARE TRAINED AFTER AUGMENTING WITH SMOTE ENN AS IT GIVES THE BEST RESULTS ON ALL MODELS COMPARED TO OTHER DATA AUGMENTATION TECHNIQUES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LxAPykXMmmM"
      },
      "source": [
        "<h1> GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8r-PhBv_Vqf"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "X=df.drop(['Potability'], axis=1)\n",
        "y=df['Potability']\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "X_train,X_test, y_train, y_test = train_test_split(X_smote_enn, y_smote_enn, random_state=42)\n",
        "\n",
        "scale = MinMaxScaler()\n",
        "X_train=scale.fit_transform(X_train)\n",
        "X_test=scale.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = []\n",
        "for i, df_visualize in enumerate(df.groupby([\"Potability\"])):\n",
        "    labels.append(df_visualize[0])\n",
        "    plt.bar(i, df_visualize[1].count(), label=df_visualize[0])\n",
        "plt.xticks(range(len(labels)), labels)\n",
        "plt.legend()\n",
        "plt.title('Potability')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g1Hl8P2MGk9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcnWGONVOOLa"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.classifier import ROCAUC\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qolOkLTK-rEE",
        "outputId": "07ed80ef-f4b1-4a2a-d603-e9031065534f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.71      0.76       119\n",
            "           1       0.80      0.87      0.83       158\n",
            "\n",
            "    accuracy                           0.80       277\n",
            "   macro avg       0.80      0.79      0.79       277\n",
            "weighted avg       0.80      0.80      0.80       277\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = GradientBoostingClassifier(n_estimators=3000)\n",
        "model.fit(X_train,y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "gb=accuracy_score(y_test,pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJt-F-Y6PU3Z"
      },
      "source": [
        "N_estimators. n_estimators represents the number of trees in the forest. Usually the higher the number of trees the better to learn the data. However, adding a lot of trees can slow down the training process considerably, therefore we do a parameter search to find the sweet spot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oXC_5niNLom"
      },
      "source": [
        "<h1> BaggingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pxukiu6_0I-",
        "outputId": "94195380-161a-4076-e3e5-427a92d87812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.68      0.71       119\n",
            "           1       0.77      0.82      0.80       158\n",
            "\n",
            "    accuracy                           0.76       277\n",
            "   macro avg       0.76      0.75      0.75       277\n",
            "weighted avg       0.76      0.76      0.76       277\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = BaggingClassifier(n_estimators=900)\n",
        "model.fit(X_train,y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "bc=accuracy_score(y_test,pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D9L4OdgNnaO"
      },
      "source": [
        "<h1> Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUpLUO82NmUH",
        "outputId": "681bbee6-649e-4d90-965b-f740f78bdfa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.49      0.56       119\n",
            "           1       0.68      0.82      0.74       158\n",
            "\n",
            "    accuracy                           0.68       277\n",
            "   macro avg       0.67      0.65      0.65       277\n",
            "weighted avg       0.67      0.68      0.66       277\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = KNeighborsClassifier(n_neighbors=9, leaf_size=20)\n",
        "model.fit(X_train,y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "dt=accuracy_score(y_test,pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqWFQT2vVlCH"
      },
      "source": [
        "<h1> RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lKFxVvWThKF",
        "outputId": "9fcacd12-2890-4d12-84a3-fac5fd3e8390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.65      0.68       119\n",
            "           1       0.75      0.80      0.78       158\n",
            "\n",
            "    accuracy                           0.74       277\n",
            "   macro avg       0.73      0.73      0.73       277\n",
            "weighted avg       0.73      0.74      0.73       277\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = RandomForestClassifier(criterion='gini',n_estimators=50)\n",
        "model.fit(X_train,y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "rf=accuracy_score(y_test,pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT53X-RuVyz5"
      },
      "source": [
        "<H1> KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knt4px-JWIhL"
      },
      "source": [
        "- The leaf size controls the minimum number of points in a given node, and effectively adjusts the tradeoff between the cost of node traversal and the cost of a brute-force distance estimate.\n",
        "\n",
        "- n_estimators represents the number of trees in the forest. Usually the higher the number of trees the better to learn the data. However, adding a lot of trees can slow down the training process considerably, therefore we do a parameter search to find the sweet spot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnfvt1hWV1kK",
        "outputId": "72c88ad2-2b8e-449b-a3b3-eb93686f40da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64       119\n",
            "           1       0.73      0.75      0.74       158\n",
            "\n",
            "    accuracy                           0.70       277\n",
            "   macro avg       0.69      0.69      0.69       277\n",
            "weighted avg       0.70      0.70      0.70       277\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = KNeighborsClassifier(n_neighbors=8, leaf_size=20)\n",
        "model.fit(X_train,y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "knn=accuracy_score(y_test,pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYI_iP-nW1zf"
      },
      "source": [
        "**Low Bias, High Variance:** When KNN uses only one neighbor (k=1), the model's decision is heavily influenced by the closest single data point. This can lead to a model that fits the training data very closely, effectively memorizing the training set. This is a classic sign of overfitting because the model may struggle to generalize to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI4UTlNTXNXP"
      },
      "source": [
        "<H1> SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDJCfax8XPm1",
        "outputId": "80fa1516-77e1-45b7-f382-5fdc277bf968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.64      0.65       119\n",
            "           1       0.73      0.75      0.74       158\n",
            "\n",
            "    accuracy                           0.70       277\n",
            "   macro avg       0.69      0.69      0.69       277\n",
            "weighted avg       0.70      0.70      0.70       277\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = SVC(kernel ='poly', degree = 8)\n",
        "model.fit(X_train,y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "svm=accuracy_score(y_test,pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4yT1F4SYb8I"
      },
      "source": [
        "The impact of the kernel on the accuracy of the SVM model depends on whether the data is linearly separable or not, the complexity of the underlying patterns, and the choice of appropriate kernel hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WgiTI_cYpNG"
      },
      "source": [
        "<H1> LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCRV5eqKYoNc",
        "outputId": "18bbca16-edea-4ab4-f33d-c01c6d9e02ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.24      0.33       119\n",
            "           1       0.59      0.84      0.70       158\n",
            "\n",
            "    accuracy                           0.58       277\n",
            "   macro avg       0.56      0.54      0.51       277\n",
            "weighted avg       0.57      0.58      0.54       277\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression(max_iter=500,random_state=0)\n",
        "model.fit(X_train,y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "lr=accuracy_score(y_test,pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzcbSo48bD81"
      },
      "source": [
        "<h1> AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ilCerv-bHEH",
        "outputId": "df8ef46b-70f6-4b02-f835-97036e6ed4f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.41      0.49       119\n",
            "           1       0.64      0.79      0.71       158\n",
            "\n",
            "    accuracy                           0.63       277\n",
            "   macro avg       0.62      0.60      0.60       277\n",
            "weighted avg       0.62      0.63      0.61       277\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = AdaBoostClassifier(learning_rate= 1,n_estimators= 20,random_state=42)\n",
        "model.fit(X_train,y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "ab=accuracy_score(y_test,pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1MFNglOWUIu"
      },
      "source": [
        "<h1> ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwotqV0dWVY0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJbYRq9HWpkl"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(20, activation='relu', input_dim=9))\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(5, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTf8r197WwxF"
      },
      "outputs": [],
      "source": [
        "opt = keras.optimizers.Adam()\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=opt,metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM8OshqTW4Ap",
        "outputId": "82ce485f-6367-4b5f-e592-f7da1a805677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1751 - accuracy: 0.7446 - val_loss: 0.2389 - val_accuracy: 0.6137\n",
            "Epoch 2/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1760 - accuracy: 0.7482 - val_loss: 0.2411 - val_accuracy: 0.6354\n",
            "Epoch 3/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.7410 - val_loss: 0.2434 - val_accuracy: 0.6245\n",
            "Epoch 4/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1768 - accuracy: 0.7470 - val_loss: 0.2434 - val_accuracy: 0.6209\n",
            "Epoch 5/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1778 - accuracy: 0.7422 - val_loss: 0.2407 - val_accuracy: 0.6282\n",
            "Epoch 6/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1766 - accuracy: 0.7386 - val_loss: 0.2454 - val_accuracy: 0.6065\n",
            "Epoch 7/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1859 - accuracy: 0.7289 - val_loss: 0.2435 - val_accuracy: 0.6137\n",
            "Epoch 8/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1814 - accuracy: 0.7361 - val_loss: 0.2437 - val_accuracy: 0.6173\n",
            "Epoch 9/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1805 - accuracy: 0.7337 - val_loss: 0.2467 - val_accuracy: 0.6209\n",
            "Epoch 10/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1802 - accuracy: 0.7313 - val_loss: 0.2379 - val_accuracy: 0.6101\n",
            "Epoch 11/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1805 - accuracy: 0.7265 - val_loss: 0.2420 - val_accuracy: 0.6282\n",
            "Epoch 12/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1775 - accuracy: 0.7349 - val_loss: 0.2427 - val_accuracy: 0.6390\n",
            "Epoch 13/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1769 - accuracy: 0.7398 - val_loss: 0.2397 - val_accuracy: 0.6209\n",
            "Epoch 14/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1760 - accuracy: 0.7325 - val_loss: 0.2448 - val_accuracy: 0.6209\n",
            "Epoch 15/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1770 - accuracy: 0.7373 - val_loss: 0.2390 - val_accuracy: 0.6209\n",
            "Epoch 16/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1760 - accuracy: 0.7458 - val_loss: 0.2406 - val_accuracy: 0.6282\n",
            "Epoch 17/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1749 - accuracy: 0.7494 - val_loss: 0.2404 - val_accuracy: 0.6282\n",
            "Epoch 18/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1754 - accuracy: 0.7542 - val_loss: 0.2428 - val_accuracy: 0.6426\n",
            "Epoch 19/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1776 - accuracy: 0.7386 - val_loss: 0.2439 - val_accuracy: 0.6354\n",
            "Epoch 20/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1767 - accuracy: 0.7470 - val_loss: 0.2436 - val_accuracy: 0.6282\n",
            "Epoch 21/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1738 - accuracy: 0.7542 - val_loss: 0.2418 - val_accuracy: 0.6318\n",
            "Epoch 22/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1767 - accuracy: 0.7373 - val_loss: 0.2458 - val_accuracy: 0.6065\n",
            "Epoch 23/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1768 - accuracy: 0.7446 - val_loss: 0.2431 - val_accuracy: 0.6318\n",
            "Epoch 24/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1763 - accuracy: 0.7373 - val_loss: 0.2419 - val_accuracy: 0.6354\n",
            "Epoch 25/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1809 - accuracy: 0.7265 - val_loss: 0.2421 - val_accuracy: 0.6209\n",
            "Epoch 26/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1764 - accuracy: 0.7410 - val_loss: 0.2420 - val_accuracy: 0.6282\n",
            "Epoch 27/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.7422 - val_loss: 0.2430 - val_accuracy: 0.6318\n",
            "Epoch 28/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1763 - accuracy: 0.7446 - val_loss: 0.2430 - val_accuracy: 0.6318\n",
            "Epoch 29/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1752 - accuracy: 0.7470 - val_loss: 0.2428 - val_accuracy: 0.6354\n",
            "Epoch 30/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1754 - accuracy: 0.7422 - val_loss: 0.2430 - val_accuracy: 0.6318\n",
            "Epoch 31/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1748 - accuracy: 0.7470 - val_loss: 0.2429 - val_accuracy: 0.6318\n",
            "Epoch 32/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1773 - accuracy: 0.7386 - val_loss: 0.2399 - val_accuracy: 0.6318\n",
            "Epoch 33/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1758 - accuracy: 0.7458 - val_loss: 0.2434 - val_accuracy: 0.6318\n",
            "Epoch 34/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1748 - accuracy: 0.7470 - val_loss: 0.2430 - val_accuracy: 0.6173\n",
            "Epoch 35/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1762 - accuracy: 0.7470 - val_loss: 0.2457 - val_accuracy: 0.6245\n",
            "Epoch 36/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1789 - accuracy: 0.7398 - val_loss: 0.2423 - val_accuracy: 0.6173\n",
            "Epoch 37/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1768 - accuracy: 0.7410 - val_loss: 0.2429 - val_accuracy: 0.6318\n",
            "Epoch 38/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.7530 - val_loss: 0.2427 - val_accuracy: 0.6245\n",
            "Epoch 39/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.7482 - val_loss: 0.2416 - val_accuracy: 0.6318\n",
            "Epoch 40/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1741 - accuracy: 0.7530 - val_loss: 0.2415 - val_accuracy: 0.6318\n",
            "Epoch 41/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1766 - accuracy: 0.7410 - val_loss: 0.2431 - val_accuracy: 0.6354\n",
            "Epoch 42/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1799 - accuracy: 0.7361 - val_loss: 0.2400 - val_accuracy: 0.6318\n",
            "Epoch 43/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1818 - accuracy: 0.7337 - val_loss: 0.2434 - val_accuracy: 0.6245\n",
            "Epoch 44/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1760 - accuracy: 0.7470 - val_loss: 0.2407 - val_accuracy: 0.6354\n",
            "Epoch 45/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1758 - accuracy: 0.7434 - val_loss: 0.2404 - val_accuracy: 0.6245\n",
            "Epoch 46/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1759 - accuracy: 0.7410 - val_loss: 0.2408 - val_accuracy: 0.6209\n",
            "Epoch 47/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1746 - accuracy: 0.7470 - val_loss: 0.2416 - val_accuracy: 0.6318\n",
            "Epoch 48/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1755 - accuracy: 0.7494 - val_loss: 0.2450 - val_accuracy: 0.6209\n",
            "Epoch 49/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.7398 - val_loss: 0.2410 - val_accuracy: 0.6245\n",
            "Epoch 50/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1765 - accuracy: 0.7446 - val_loss: 0.2398 - val_accuracy: 0.6390\n",
            "Epoch 51/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1809 - accuracy: 0.7253 - val_loss: 0.2452 - val_accuracy: 0.6318\n",
            "Epoch 52/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1811 - accuracy: 0.7265 - val_loss: 0.2438 - val_accuracy: 0.6282\n",
            "Epoch 53/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1753 - accuracy: 0.7422 - val_loss: 0.2413 - val_accuracy: 0.6209\n",
            "Epoch 54/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1757 - accuracy: 0.7506 - val_loss: 0.2453 - val_accuracy: 0.6209\n",
            "Epoch 55/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1750 - accuracy: 0.7325 - val_loss: 0.2448 - val_accuracy: 0.6137\n",
            "Epoch 56/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1746 - accuracy: 0.7494 - val_loss: 0.2417 - val_accuracy: 0.6173\n",
            "Epoch 57/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.7482 - val_loss: 0.2448 - val_accuracy: 0.6282\n",
            "Epoch 58/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1775 - accuracy: 0.7434 - val_loss: 0.2414 - val_accuracy: 0.6282\n",
            "Epoch 59/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1766 - accuracy: 0.7470 - val_loss: 0.2444 - val_accuracy: 0.6354\n",
            "Epoch 60/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1756 - accuracy: 0.7422 - val_loss: 0.2426 - val_accuracy: 0.6282\n",
            "Epoch 61/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.7458 - val_loss: 0.2416 - val_accuracy: 0.6245\n",
            "Epoch 62/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1744 - accuracy: 0.7482 - val_loss: 0.2452 - val_accuracy: 0.6282\n",
            "Epoch 63/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1755 - accuracy: 0.7458 - val_loss: 0.2404 - val_accuracy: 0.6245\n",
            "Epoch 64/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.7506 - val_loss: 0.2407 - val_accuracy: 0.6354\n",
            "Epoch 65/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.7494 - val_loss: 0.2435 - val_accuracy: 0.6426\n",
            "Epoch 66/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1748 - accuracy: 0.7518 - val_loss: 0.2414 - val_accuracy: 0.6354\n",
            "Epoch 67/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1736 - accuracy: 0.7530 - val_loss: 0.2436 - val_accuracy: 0.6282\n",
            "Epoch 68/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1731 - accuracy: 0.7494 - val_loss: 0.2433 - val_accuracy: 0.6462\n",
            "Epoch 69/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1732 - accuracy: 0.7482 - val_loss: 0.2413 - val_accuracy: 0.6318\n",
            "Epoch 70/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.7434 - val_loss: 0.2411 - val_accuracy: 0.6245\n",
            "Epoch 71/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1728 - accuracy: 0.7446 - val_loss: 0.2424 - val_accuracy: 0.6245\n",
            "Epoch 72/900\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1752 - accuracy: 0.7398 - val_loss: 0.2440 - val_accuracy: 0.6101\n",
            "Epoch 73/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1860 - accuracy: 0.7289 - val_loss: 0.2431 - val_accuracy: 0.6282\n",
            "Epoch 74/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1745 - accuracy: 0.7410 - val_loss: 0.2431 - val_accuracy: 0.6245\n",
            "Epoch 75/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1737 - accuracy: 0.7518 - val_loss: 0.2400 - val_accuracy: 0.6209\n",
            "Epoch 76/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1746 - accuracy: 0.7470 - val_loss: 0.2455 - val_accuracy: 0.6245\n",
            "Epoch 77/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1754 - accuracy: 0.7542 - val_loss: 0.2413 - val_accuracy: 0.6390\n",
            "Epoch 78/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1746 - accuracy: 0.7554 - val_loss: 0.2441 - val_accuracy: 0.6245\n",
            "Epoch 79/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1784 - accuracy: 0.7410 - val_loss: 0.2388 - val_accuracy: 0.6390\n",
            "Epoch 80/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1774 - accuracy: 0.7458 - val_loss: 0.2408 - val_accuracy: 0.6318\n",
            "Epoch 81/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.7446 - val_loss: 0.2418 - val_accuracy: 0.6282\n",
            "Epoch 82/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.7398 - val_loss: 0.2401 - val_accuracy: 0.6354\n",
            "Epoch 83/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1775 - accuracy: 0.7289 - val_loss: 0.2438 - val_accuracy: 0.6245\n",
            "Epoch 84/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1734 - accuracy: 0.7518 - val_loss: 0.2414 - val_accuracy: 0.6318\n",
            "Epoch 85/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1749 - accuracy: 0.7482 - val_loss: 0.2423 - val_accuracy: 0.6354\n",
            "Epoch 86/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1733 - accuracy: 0.7494 - val_loss: 0.2415 - val_accuracy: 0.6318\n",
            "Epoch 87/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1735 - accuracy: 0.7530 - val_loss: 0.2410 - val_accuracy: 0.6318\n",
            "Epoch 88/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.7398 - val_loss: 0.2430 - val_accuracy: 0.6354\n",
            "Epoch 89/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1741 - accuracy: 0.7470 - val_loss: 0.2437 - val_accuracy: 0.6209\n",
            "Epoch 90/900\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.7361 - val_loss: 0.2430 - val_accuracy: 0.6318\n",
            "Epoch 91/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1753 - accuracy: 0.7386 - val_loss: 0.2408 - val_accuracy: 0.6318\n",
            "Epoch 92/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1730 - accuracy: 0.7470 - val_loss: 0.2416 - val_accuracy: 0.6318\n",
            "Epoch 93/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1745 - accuracy: 0.7482 - val_loss: 0.2428 - val_accuracy: 0.6318\n",
            "Epoch 94/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1765 - accuracy: 0.7386 - val_loss: 0.2411 - val_accuracy: 0.6245\n",
            "Epoch 95/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1749 - accuracy: 0.7386 - val_loss: 0.2390 - val_accuracy: 0.6390\n",
            "Epoch 96/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.7458 - val_loss: 0.2443 - val_accuracy: 0.6173\n",
            "Epoch 97/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1762 - accuracy: 0.7301 - val_loss: 0.2397 - val_accuracy: 0.6245\n",
            "Epoch 98/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.7422 - val_loss: 0.2394 - val_accuracy: 0.6282\n",
            "Epoch 99/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.7446 - val_loss: 0.2397 - val_accuracy: 0.6426\n",
            "Epoch 100/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1753 - accuracy: 0.7313 - val_loss: 0.2464 - val_accuracy: 0.6245\n",
            "Epoch 101/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1746 - accuracy: 0.7361 - val_loss: 0.2400 - val_accuracy: 0.6282\n",
            "Epoch 102/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1732 - accuracy: 0.7482 - val_loss: 0.2450 - val_accuracy: 0.6354\n",
            "Epoch 103/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1794 - accuracy: 0.7349 - val_loss: 0.2415 - val_accuracy: 0.6462\n",
            "Epoch 104/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1733 - accuracy: 0.7530 - val_loss: 0.2443 - val_accuracy: 0.6318\n",
            "Epoch 105/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1726 - accuracy: 0.7530 - val_loss: 0.2450 - val_accuracy: 0.6245\n",
            "Epoch 106/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1737 - accuracy: 0.7506 - val_loss: 0.2412 - val_accuracy: 0.6173\n",
            "Epoch 107/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1736 - accuracy: 0.7566 - val_loss: 0.2426 - val_accuracy: 0.6498\n",
            "Epoch 108/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.7458 - val_loss: 0.2432 - val_accuracy: 0.6245\n",
            "Epoch 109/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1730 - accuracy: 0.7446 - val_loss: 0.2438 - val_accuracy: 0.6245\n",
            "Epoch 110/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1819 - accuracy: 0.7337 - val_loss: 0.2409 - val_accuracy: 0.6462\n",
            "Epoch 111/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1758 - accuracy: 0.7386 - val_loss: 0.2417 - val_accuracy: 0.6318\n",
            "Epoch 112/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1741 - accuracy: 0.7446 - val_loss: 0.2416 - val_accuracy: 0.6318\n",
            "Epoch 113/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1764 - accuracy: 0.7434 - val_loss: 0.2431 - val_accuracy: 0.6426\n",
            "Epoch 114/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1717 - accuracy: 0.7458 - val_loss: 0.2425 - val_accuracy: 0.6390\n",
            "Epoch 115/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1803 - accuracy: 0.7313 - val_loss: 0.2402 - val_accuracy: 0.6137\n",
            "Epoch 116/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1810 - accuracy: 0.7301 - val_loss: 0.2438 - val_accuracy: 0.6318\n",
            "Epoch 117/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1749 - accuracy: 0.7494 - val_loss: 0.2393 - val_accuracy: 0.6209\n",
            "Epoch 118/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.7482 - val_loss: 0.2419 - val_accuracy: 0.6390\n",
            "Epoch 119/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1731 - accuracy: 0.7566 - val_loss: 0.2416 - val_accuracy: 0.6426\n",
            "Epoch 120/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1812 - accuracy: 0.7410 - val_loss: 0.2391 - val_accuracy: 0.6318\n",
            "Epoch 121/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.7446 - val_loss: 0.2400 - val_accuracy: 0.6426\n",
            "Epoch 122/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.7458 - val_loss: 0.2450 - val_accuracy: 0.6390\n",
            "Epoch 123/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1741 - accuracy: 0.7446 - val_loss: 0.2402 - val_accuracy: 0.6318\n",
            "Epoch 124/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1721 - accuracy: 0.7518 - val_loss: 0.2431 - val_accuracy: 0.6390\n",
            "Epoch 125/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1718 - accuracy: 0.7506 - val_loss: 0.2409 - val_accuracy: 0.6390\n",
            "Epoch 126/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1725 - accuracy: 0.7530 - val_loss: 0.2422 - val_accuracy: 0.6318\n",
            "Epoch 127/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1722 - accuracy: 0.7494 - val_loss: 0.2470 - val_accuracy: 0.6282\n",
            "Epoch 128/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1823 - accuracy: 0.7253 - val_loss: 0.2415 - val_accuracy: 0.6245\n",
            "Epoch 129/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.7482 - val_loss: 0.2410 - val_accuracy: 0.6318\n",
            "Epoch 130/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1751 - accuracy: 0.7458 - val_loss: 0.2406 - val_accuracy: 0.6282\n",
            "Epoch 131/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1744 - accuracy: 0.7422 - val_loss: 0.2399 - val_accuracy: 0.6245\n",
            "Epoch 132/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.7422 - val_loss: 0.2400 - val_accuracy: 0.6390\n",
            "Epoch 133/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1718 - accuracy: 0.7542 - val_loss: 0.2429 - val_accuracy: 0.6390\n",
            "Epoch 134/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1732 - accuracy: 0.7482 - val_loss: 0.2399 - val_accuracy: 0.6282\n",
            "Epoch 135/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1733 - accuracy: 0.7422 - val_loss: 0.2399 - val_accuracy: 0.6390\n",
            "Epoch 136/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1773 - accuracy: 0.7458 - val_loss: 0.2460 - val_accuracy: 0.6282\n",
            "Epoch 137/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1750 - accuracy: 0.7349 - val_loss: 0.2437 - val_accuracy: 0.6318\n",
            "Epoch 138/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1714 - accuracy: 0.7566 - val_loss: 0.2420 - val_accuracy: 0.6498\n",
            "Epoch 139/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1714 - accuracy: 0.7554 - val_loss: 0.2396 - val_accuracy: 0.6354\n",
            "Epoch 140/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1712 - accuracy: 0.7542 - val_loss: 0.2407 - val_accuracy: 0.6390\n",
            "Epoch 141/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1714 - accuracy: 0.7458 - val_loss: 0.2406 - val_accuracy: 0.6354\n",
            "Epoch 142/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1728 - accuracy: 0.7530 - val_loss: 0.2433 - val_accuracy: 0.6462\n",
            "Epoch 143/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1725 - accuracy: 0.7494 - val_loss: 0.2447 - val_accuracy: 0.6245\n",
            "Epoch 144/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1729 - accuracy: 0.7398 - val_loss: 0.2418 - val_accuracy: 0.6318\n",
            "Epoch 145/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1725 - accuracy: 0.7470 - val_loss: 0.2436 - val_accuracy: 0.6282\n",
            "Epoch 146/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1737 - accuracy: 0.7566 - val_loss: 0.2408 - val_accuracy: 0.6318\n",
            "Epoch 147/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.7482 - val_loss: 0.2423 - val_accuracy: 0.6282\n",
            "Epoch 148/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.7518 - val_loss: 0.2429 - val_accuracy: 0.6390\n",
            "Epoch 149/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1720 - accuracy: 0.7482 - val_loss: 0.2418 - val_accuracy: 0.6245\n",
            "Epoch 150/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1736 - accuracy: 0.7458 - val_loss: 0.2405 - val_accuracy: 0.6282\n",
            "Epoch 151/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1721 - accuracy: 0.7446 - val_loss: 0.2458 - val_accuracy: 0.6318\n",
            "Epoch 152/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1715 - accuracy: 0.7542 - val_loss: 0.2413 - val_accuracy: 0.6390\n",
            "Epoch 153/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1702 - accuracy: 0.7542 - val_loss: 0.2447 - val_accuracy: 0.6390\n",
            "Epoch 154/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1709 - accuracy: 0.7530 - val_loss: 0.2425 - val_accuracy: 0.6354\n",
            "Epoch 155/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1715 - accuracy: 0.7518 - val_loss: 0.2398 - val_accuracy: 0.6462\n",
            "Epoch 156/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.7482 - val_loss: 0.2463 - val_accuracy: 0.6318\n",
            "Epoch 157/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1722 - accuracy: 0.7470 - val_loss: 0.2428 - val_accuracy: 0.6173\n",
            "Epoch 158/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1789 - accuracy: 0.7277 - val_loss: 0.2441 - val_accuracy: 0.6318\n",
            "Epoch 159/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1826 - accuracy: 0.7337 - val_loss: 0.2461 - val_accuracy: 0.6245\n",
            "Epoch 160/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1754 - accuracy: 0.7446 - val_loss: 0.2415 - val_accuracy: 0.6462\n",
            "Epoch 161/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.7506 - val_loss: 0.2428 - val_accuracy: 0.6390\n",
            "Epoch 162/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.7578 - val_loss: 0.2447 - val_accuracy: 0.6354\n",
            "Epoch 163/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1720 - accuracy: 0.7530 - val_loss: 0.2484 - val_accuracy: 0.6282\n",
            "Epoch 164/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1746 - accuracy: 0.7494 - val_loss: 0.2426 - val_accuracy: 0.6426\n",
            "Epoch 165/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1725 - accuracy: 0.7554 - val_loss: 0.2418 - val_accuracy: 0.6426\n",
            "Epoch 166/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1778 - accuracy: 0.7398 - val_loss: 0.2505 - val_accuracy: 0.6101\n",
            "Epoch 167/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1741 - accuracy: 0.7434 - val_loss: 0.2449 - val_accuracy: 0.6282\n",
            "Epoch 168/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1747 - accuracy: 0.7422 - val_loss: 0.2446 - val_accuracy: 0.6245\n",
            "Epoch 169/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1732 - accuracy: 0.7422 - val_loss: 0.2448 - val_accuracy: 0.6318\n",
            "Epoch 170/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1713 - accuracy: 0.7470 - val_loss: 0.2431 - val_accuracy: 0.6426\n",
            "Epoch 171/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1707 - accuracy: 0.7554 - val_loss: 0.2439 - val_accuracy: 0.6534\n",
            "Epoch 172/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1722 - accuracy: 0.7398 - val_loss: 0.2448 - val_accuracy: 0.6282\n",
            "Epoch 173/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1734 - accuracy: 0.7542 - val_loss: 0.2412 - val_accuracy: 0.6354\n",
            "Epoch 174/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1752 - accuracy: 0.7446 - val_loss: 0.2409 - val_accuracy: 0.6282\n",
            "Epoch 175/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1721 - accuracy: 0.7506 - val_loss: 0.2419 - val_accuracy: 0.6245\n",
            "Epoch 176/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1756 - accuracy: 0.7386 - val_loss: 0.2454 - val_accuracy: 0.6173\n",
            "Epoch 177/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1733 - accuracy: 0.7494 - val_loss: 0.2422 - val_accuracy: 0.6173\n",
            "Epoch 178/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1706 - accuracy: 0.7530 - val_loss: 0.2418 - val_accuracy: 0.6209\n",
            "Epoch 179/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1717 - accuracy: 0.7470 - val_loss: 0.2411 - val_accuracy: 0.6390\n",
            "Epoch 180/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1715 - accuracy: 0.7470 - val_loss: 0.2436 - val_accuracy: 0.6282\n",
            "Epoch 181/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1742 - accuracy: 0.7530 - val_loss: 0.2406 - val_accuracy: 0.6318\n",
            "Epoch 182/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1696 - accuracy: 0.7518 - val_loss: 0.2401 - val_accuracy: 0.6282\n",
            "Epoch 183/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1731 - accuracy: 0.7554 - val_loss: 0.2411 - val_accuracy: 0.6354\n",
            "Epoch 184/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1724 - accuracy: 0.7554 - val_loss: 0.2422 - val_accuracy: 0.6245\n",
            "Epoch 185/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1712 - accuracy: 0.7542 - val_loss: 0.2413 - val_accuracy: 0.6282\n",
            "Epoch 186/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1705 - accuracy: 0.7482 - val_loss: 0.2408 - val_accuracy: 0.6354\n",
            "Epoch 187/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1712 - accuracy: 0.7542 - val_loss: 0.2448 - val_accuracy: 0.6282\n",
            "Epoch 188/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1700 - accuracy: 0.7614 - val_loss: 0.2417 - val_accuracy: 0.6282\n",
            "Epoch 189/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1700 - accuracy: 0.7482 - val_loss: 0.2413 - val_accuracy: 0.6245\n",
            "Epoch 190/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1705 - accuracy: 0.7494 - val_loss: 0.2431 - val_accuracy: 0.6354\n",
            "Epoch 191/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1701 - accuracy: 0.7627 - val_loss: 0.2425 - val_accuracy: 0.6426\n",
            "Epoch 192/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.7602 - val_loss: 0.2415 - val_accuracy: 0.6318\n",
            "Epoch 193/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1734 - accuracy: 0.7530 - val_loss: 0.2426 - val_accuracy: 0.6282\n",
            "Epoch 194/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1721 - accuracy: 0.7542 - val_loss: 0.2420 - val_accuracy: 0.6354\n",
            "Epoch 195/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1705 - accuracy: 0.7639 - val_loss: 0.2457 - val_accuracy: 0.6282\n",
            "Epoch 196/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1703 - accuracy: 0.7663 - val_loss: 0.2411 - val_accuracy: 0.6354\n",
            "Epoch 197/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1707 - accuracy: 0.7530 - val_loss: 0.2421 - val_accuracy: 0.6390\n",
            "Epoch 198/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.7554 - val_loss: 0.2400 - val_accuracy: 0.6534\n",
            "Epoch 199/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1715 - accuracy: 0.7518 - val_loss: 0.2424 - val_accuracy: 0.6318\n",
            "Epoch 200/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1690 - accuracy: 0.7578 - val_loss: 0.2427 - val_accuracy: 0.6245\n",
            "Epoch 201/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1697 - accuracy: 0.7542 - val_loss: 0.2408 - val_accuracy: 0.6390\n",
            "Epoch 202/900\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.7542 - val_loss: 0.2403 - val_accuracy: 0.6390\n",
            "Epoch 203/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1701 - accuracy: 0.7470 - val_loss: 0.2437 - val_accuracy: 0.6245\n",
            "Epoch 204/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1759 - accuracy: 0.7434 - val_loss: 0.2431 - val_accuracy: 0.6318\n",
            "Epoch 205/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1701 - accuracy: 0.7590 - val_loss: 0.2406 - val_accuracy: 0.6462\n",
            "Epoch 206/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1732 - accuracy: 0.7506 - val_loss: 0.2417 - val_accuracy: 0.6354\n",
            "Epoch 207/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1714 - accuracy: 0.7506 - val_loss: 0.2479 - val_accuracy: 0.6245\n",
            "Epoch 208/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1720 - accuracy: 0.7530 - val_loss: 0.2426 - val_accuracy: 0.6282\n",
            "Epoch 209/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1695 - accuracy: 0.7530 - val_loss: 0.2407 - val_accuracy: 0.6318\n",
            "Epoch 210/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1694 - accuracy: 0.7518 - val_loss: 0.2416 - val_accuracy: 0.6390\n",
            "Epoch 211/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1807 - accuracy: 0.7373 - val_loss: 0.2485 - val_accuracy: 0.6318\n",
            "Epoch 212/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1743 - accuracy: 0.7530 - val_loss: 0.2415 - val_accuracy: 0.6282\n",
            "Epoch 213/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1694 - accuracy: 0.7542 - val_loss: 0.2420 - val_accuracy: 0.6282\n",
            "Epoch 214/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1709 - accuracy: 0.7542 - val_loss: 0.2407 - val_accuracy: 0.6354\n",
            "Epoch 215/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1693 - accuracy: 0.7590 - val_loss: 0.2446 - val_accuracy: 0.6318\n",
            "Epoch 216/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1690 - accuracy: 0.7530 - val_loss: 0.2438 - val_accuracy: 0.6390\n",
            "Epoch 217/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1684 - accuracy: 0.7578 - val_loss: 0.2412 - val_accuracy: 0.6354\n",
            "Epoch 218/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1703 - accuracy: 0.7530 - val_loss: 0.2424 - val_accuracy: 0.6282\n",
            "Epoch 219/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1690 - accuracy: 0.7530 - val_loss: 0.2405 - val_accuracy: 0.6354\n",
            "Epoch 220/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1708 - accuracy: 0.7518 - val_loss: 0.2451 - val_accuracy: 0.6390\n",
            "Epoch 221/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1708 - accuracy: 0.7470 - val_loss: 0.2466 - val_accuracy: 0.6318\n",
            "Epoch 222/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1688 - accuracy: 0.7566 - val_loss: 0.2428 - val_accuracy: 0.6282\n",
            "Epoch 223/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.7482 - val_loss: 0.2425 - val_accuracy: 0.6354\n",
            "Epoch 224/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1709 - accuracy: 0.7494 - val_loss: 0.2431 - val_accuracy: 0.6282\n",
            "Epoch 225/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1682 - accuracy: 0.7566 - val_loss: 0.2428 - val_accuracy: 0.6354\n",
            "Epoch 226/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1721 - accuracy: 0.7566 - val_loss: 0.2458 - val_accuracy: 0.6354\n",
            "Epoch 227/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.7458 - val_loss: 0.2476 - val_accuracy: 0.6318\n",
            "Epoch 228/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1723 - accuracy: 0.7482 - val_loss: 0.2428 - val_accuracy: 0.6390\n",
            "Epoch 229/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1720 - accuracy: 0.7566 - val_loss: 0.2431 - val_accuracy: 0.6318\n",
            "Epoch 230/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.7590 - val_loss: 0.2426 - val_accuracy: 0.6354\n",
            "Epoch 231/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1685 - accuracy: 0.7590 - val_loss: 0.2421 - val_accuracy: 0.6462\n",
            "Epoch 232/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.7566 - val_loss: 0.2414 - val_accuracy: 0.6209\n",
            "Epoch 233/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1741 - accuracy: 0.7470 - val_loss: 0.2428 - val_accuracy: 0.6390\n",
            "Epoch 234/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1731 - accuracy: 0.7518 - val_loss: 0.2435 - val_accuracy: 0.6209\n",
            "Epoch 235/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1704 - accuracy: 0.7482 - val_loss: 0.2422 - val_accuracy: 0.6390\n",
            "Epoch 236/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1692 - accuracy: 0.7590 - val_loss: 0.2458 - val_accuracy: 0.6390\n",
            "Epoch 237/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1689 - accuracy: 0.7578 - val_loss: 0.2425 - val_accuracy: 0.6462\n",
            "Epoch 238/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1708 - accuracy: 0.7566 - val_loss: 0.2480 - val_accuracy: 0.6426\n",
            "Epoch 239/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.7614 - val_loss: 0.2420 - val_accuracy: 0.6354\n",
            "Epoch 240/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.7506 - val_loss: 0.2485 - val_accuracy: 0.6354\n",
            "Epoch 241/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1729 - accuracy: 0.7458 - val_loss: 0.2466 - val_accuracy: 0.6245\n",
            "Epoch 242/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1710 - accuracy: 0.7506 - val_loss: 0.2420 - val_accuracy: 0.6282\n",
            "Epoch 243/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1705 - accuracy: 0.7554 - val_loss: 0.2472 - val_accuracy: 0.6318\n",
            "Epoch 244/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1694 - accuracy: 0.7518 - val_loss: 0.2434 - val_accuracy: 0.6354\n",
            "Epoch 245/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1704 - accuracy: 0.7590 - val_loss: 0.2438 - val_accuracy: 0.6390\n",
            "Epoch 246/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1703 - accuracy: 0.7566 - val_loss: 0.2436 - val_accuracy: 0.6462\n",
            "Epoch 247/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1696 - accuracy: 0.7639 - val_loss: 0.2450 - val_accuracy: 0.6173\n",
            "Epoch 248/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1700 - accuracy: 0.7518 - val_loss: 0.2508 - val_accuracy: 0.6209\n",
            "Epoch 249/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1726 - accuracy: 0.7482 - val_loss: 0.2412 - val_accuracy: 0.6318\n",
            "Epoch 250/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1716 - accuracy: 0.7506 - val_loss: 0.2414 - val_accuracy: 0.6426\n",
            "Epoch 251/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1797 - accuracy: 0.7325 - val_loss: 0.2439 - val_accuracy: 0.6282\n",
            "Epoch 252/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1778 - accuracy: 0.7373 - val_loss: 0.2454 - val_accuracy: 0.6354\n",
            "Epoch 253/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1707 - accuracy: 0.7494 - val_loss: 0.2466 - val_accuracy: 0.6282\n",
            "Epoch 254/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1835 - accuracy: 0.7289 - val_loss: 0.2398 - val_accuracy: 0.6390\n",
            "Epoch 255/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1747 - accuracy: 0.7446 - val_loss: 0.2412 - val_accuracy: 0.6390\n",
            "Epoch 256/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1701 - accuracy: 0.7639 - val_loss: 0.2436 - val_accuracy: 0.6390\n",
            "Epoch 257/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1752 - accuracy: 0.7446 - val_loss: 0.2418 - val_accuracy: 0.6354\n",
            "Epoch 258/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1699 - accuracy: 0.7590 - val_loss: 0.2430 - val_accuracy: 0.6318\n",
            "Epoch 259/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1692 - accuracy: 0.7530 - val_loss: 0.2449 - val_accuracy: 0.6245\n",
            "Epoch 260/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1705 - accuracy: 0.7542 - val_loss: 0.2402 - val_accuracy: 0.6426\n",
            "Epoch 261/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1683 - accuracy: 0.7578 - val_loss: 0.2430 - val_accuracy: 0.6318\n",
            "Epoch 262/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1715 - accuracy: 0.7530 - val_loss: 0.2404 - val_accuracy: 0.6462\n",
            "Epoch 263/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1734 - accuracy: 0.7470 - val_loss: 0.2415 - val_accuracy: 0.6426\n",
            "Epoch 264/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1708 - accuracy: 0.7494 - val_loss: 0.2461 - val_accuracy: 0.6209\n",
            "Epoch 265/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1723 - accuracy: 0.7506 - val_loss: 0.2423 - val_accuracy: 0.6498\n",
            "Epoch 266/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1680 - accuracy: 0.7627 - val_loss: 0.2462 - val_accuracy: 0.6354\n",
            "Epoch 267/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1690 - accuracy: 0.7542 - val_loss: 0.2428 - val_accuracy: 0.6390\n",
            "Epoch 268/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1686 - accuracy: 0.7566 - val_loss: 0.2425 - val_accuracy: 0.6282\n",
            "Epoch 269/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1700 - accuracy: 0.7506 - val_loss: 0.2445 - val_accuracy: 0.6318\n",
            "Epoch 270/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1694 - accuracy: 0.7590 - val_loss: 0.2418 - val_accuracy: 0.6462\n",
            "Epoch 271/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1688 - accuracy: 0.7578 - val_loss: 0.2450 - val_accuracy: 0.6318\n",
            "Epoch 272/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1677 - accuracy: 0.7590 - val_loss: 0.2419 - val_accuracy: 0.6390\n",
            "Epoch 273/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1682 - accuracy: 0.7578 - val_loss: 0.2457 - val_accuracy: 0.6390\n",
            "Epoch 274/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1682 - accuracy: 0.7566 - val_loss: 0.2461 - val_accuracy: 0.6318\n",
            "Epoch 275/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1681 - accuracy: 0.7554 - val_loss: 0.2427 - val_accuracy: 0.6462\n",
            "Epoch 276/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1676 - accuracy: 0.7639 - val_loss: 0.2444 - val_accuracy: 0.6354\n",
            "Epoch 277/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1682 - accuracy: 0.7651 - val_loss: 0.2426 - val_accuracy: 0.6354\n",
            "Epoch 278/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1679 - accuracy: 0.7566 - val_loss: 0.2455 - val_accuracy: 0.6354\n",
            "Epoch 279/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1702 - accuracy: 0.7627 - val_loss: 0.2462 - val_accuracy: 0.6354\n",
            "Epoch 280/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1724 - accuracy: 0.7554 - val_loss: 0.2418 - val_accuracy: 0.6426\n",
            "Epoch 281/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1706 - accuracy: 0.7530 - val_loss: 0.2448 - val_accuracy: 0.6390\n",
            "Epoch 282/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1675 - accuracy: 0.7554 - val_loss: 0.2410 - val_accuracy: 0.6390\n",
            "Epoch 283/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1690 - accuracy: 0.7566 - val_loss: 0.2442 - val_accuracy: 0.6282\n",
            "Epoch 284/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1676 - accuracy: 0.7590 - val_loss: 0.2423 - val_accuracy: 0.6498\n",
            "Epoch 285/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1755 - accuracy: 0.7542 - val_loss: 0.2405 - val_accuracy: 0.6282\n",
            "Epoch 286/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1704 - accuracy: 0.7518 - val_loss: 0.2385 - val_accuracy: 0.6462\n",
            "Epoch 287/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1688 - accuracy: 0.7566 - val_loss: 0.2414 - val_accuracy: 0.6354\n",
            "Epoch 288/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1678 - accuracy: 0.7566 - val_loss: 0.2482 - val_accuracy: 0.6354\n",
            "Epoch 289/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1714 - accuracy: 0.7494 - val_loss: 0.2416 - val_accuracy: 0.6462\n",
            "Epoch 290/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1722 - accuracy: 0.7458 - val_loss: 0.2495 - val_accuracy: 0.6318\n",
            "Epoch 291/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1697 - accuracy: 0.7542 - val_loss: 0.2415 - val_accuracy: 0.6390\n",
            "Epoch 292/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1680 - accuracy: 0.7627 - val_loss: 0.2424 - val_accuracy: 0.6318\n",
            "Epoch 293/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1700 - accuracy: 0.7566 - val_loss: 0.2407 - val_accuracy: 0.6390\n",
            "Epoch 294/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1684 - accuracy: 0.7627 - val_loss: 0.2408 - val_accuracy: 0.6390\n",
            "Epoch 295/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1693 - accuracy: 0.7590 - val_loss: 0.2429 - val_accuracy: 0.6426\n",
            "Epoch 296/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1672 - accuracy: 0.7614 - val_loss: 0.2422 - val_accuracy: 0.6354\n",
            "Epoch 297/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1663 - accuracy: 0.7614 - val_loss: 0.2431 - val_accuracy: 0.6354\n",
            "Epoch 298/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1680 - accuracy: 0.7590 - val_loss: 0.2452 - val_accuracy: 0.6426\n",
            "Epoch 299/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1664 - accuracy: 0.7602 - val_loss: 0.2451 - val_accuracy: 0.6354\n",
            "Epoch 300/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1665 - accuracy: 0.7723 - val_loss: 0.2431 - val_accuracy: 0.6426\n",
            "Epoch 301/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1696 - accuracy: 0.7494 - val_loss: 0.2428 - val_accuracy: 0.6534\n",
            "Epoch 302/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1714 - accuracy: 0.7506 - val_loss: 0.2448 - val_accuracy: 0.6318\n",
            "Epoch 303/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1686 - accuracy: 0.7554 - val_loss: 0.2425 - val_accuracy: 0.6462\n",
            "Epoch 304/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1662 - accuracy: 0.7614 - val_loss: 0.2421 - val_accuracy: 0.6390\n",
            "Epoch 305/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1670 - accuracy: 0.7578 - val_loss: 0.2490 - val_accuracy: 0.6101\n",
            "Epoch 306/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1739 - accuracy: 0.7446 - val_loss: 0.2416 - val_accuracy: 0.6426\n",
            "Epoch 307/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1667 - accuracy: 0.7675 - val_loss: 0.2419 - val_accuracy: 0.6354\n",
            "Epoch 308/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1665 - accuracy: 0.7590 - val_loss: 0.2405 - val_accuracy: 0.6606\n",
            "Epoch 309/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1654 - accuracy: 0.7627 - val_loss: 0.2446 - val_accuracy: 0.6354\n",
            "Epoch 310/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1671 - accuracy: 0.7614 - val_loss: 0.2462 - val_accuracy: 0.6354\n",
            "Epoch 311/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1678 - accuracy: 0.7578 - val_loss: 0.2419 - val_accuracy: 0.6462\n",
            "Epoch 312/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1665 - accuracy: 0.7651 - val_loss: 0.2404 - val_accuracy: 0.6498\n",
            "Epoch 313/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1669 - accuracy: 0.7566 - val_loss: 0.2418 - val_accuracy: 0.6462\n",
            "Epoch 314/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1672 - accuracy: 0.7663 - val_loss: 0.2421 - val_accuracy: 0.6318\n",
            "Epoch 315/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1730 - accuracy: 0.7494 - val_loss: 0.2452 - val_accuracy: 0.6282\n",
            "Epoch 316/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1675 - accuracy: 0.7614 - val_loss: 0.2450 - val_accuracy: 0.6462\n",
            "Epoch 317/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1727 - accuracy: 0.7410 - val_loss: 0.2425 - val_accuracy: 0.6426\n",
            "Epoch 318/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1704 - accuracy: 0.7518 - val_loss: 0.2428 - val_accuracy: 0.6462\n",
            "Epoch 319/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1648 - accuracy: 0.7663 - val_loss: 0.2431 - val_accuracy: 0.6282\n",
            "Epoch 320/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1660 - accuracy: 0.7578 - val_loss: 0.2436 - val_accuracy: 0.6390\n",
            "Epoch 321/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1709 - accuracy: 0.7530 - val_loss: 0.2451 - val_accuracy: 0.6426\n",
            "Epoch 322/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1689 - accuracy: 0.7566 - val_loss: 0.2395 - val_accuracy: 0.6570\n",
            "Epoch 323/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1713 - accuracy: 0.7530 - val_loss: 0.2411 - val_accuracy: 0.6354\n",
            "Epoch 324/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1684 - accuracy: 0.7554 - val_loss: 0.2439 - val_accuracy: 0.6354\n",
            "Epoch 325/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1686 - accuracy: 0.7675 - val_loss: 0.2396 - val_accuracy: 0.6534\n",
            "Epoch 326/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1668 - accuracy: 0.7614 - val_loss: 0.2463 - val_accuracy: 0.6426\n",
            "Epoch 327/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1675 - accuracy: 0.7614 - val_loss: 0.2418 - val_accuracy: 0.6318\n",
            "Epoch 328/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1657 - accuracy: 0.7747 - val_loss: 0.2409 - val_accuracy: 0.6354\n",
            "Epoch 329/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1655 - accuracy: 0.7675 - val_loss: 0.2453 - val_accuracy: 0.6318\n",
            "Epoch 330/900\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.7663 - val_loss: 0.2457 - val_accuracy: 0.6245\n",
            "Epoch 331/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.7639 - val_loss: 0.2454 - val_accuracy: 0.6282\n",
            "Epoch 332/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.7614 - val_loss: 0.2432 - val_accuracy: 0.6426\n",
            "Epoch 333/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1754 - accuracy: 0.7410 - val_loss: 0.2442 - val_accuracy: 0.6318\n",
            "Epoch 334/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1684 - accuracy: 0.7578 - val_loss: 0.2422 - val_accuracy: 0.6570\n",
            "Epoch 335/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.7590 - val_loss: 0.2434 - val_accuracy: 0.6282\n",
            "Epoch 336/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1673 - accuracy: 0.7614 - val_loss: 0.2427 - val_accuracy: 0.6570\n",
            "Epoch 337/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1704 - accuracy: 0.7542 - val_loss: 0.2420 - val_accuracy: 0.6426\n",
            "Epoch 338/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1700 - accuracy: 0.7554 - val_loss: 0.2487 - val_accuracy: 0.6318\n",
            "Epoch 339/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1680 - accuracy: 0.7651 - val_loss: 0.2430 - val_accuracy: 0.6390\n",
            "Epoch 340/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1644 - accuracy: 0.7771 - val_loss: 0.2419 - val_accuracy: 0.6426\n",
            "Epoch 341/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1665 - accuracy: 0.7590 - val_loss: 0.2427 - val_accuracy: 0.6390\n",
            "Epoch 342/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1654 - accuracy: 0.7627 - val_loss: 0.2434 - val_accuracy: 0.6498\n",
            "Epoch 343/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1662 - accuracy: 0.7675 - val_loss: 0.2426 - val_accuracy: 0.6534\n",
            "Epoch 344/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1651 - accuracy: 0.7699 - val_loss: 0.2437 - val_accuracy: 0.6282\n",
            "Epoch 345/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1685 - accuracy: 0.7542 - val_loss: 0.2403 - val_accuracy: 0.6426\n",
            "Epoch 346/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1683 - accuracy: 0.7651 - val_loss: 0.2406 - val_accuracy: 0.6498\n",
            "Epoch 347/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1749 - accuracy: 0.7325 - val_loss: 0.2444 - val_accuracy: 0.6390\n",
            "Epoch 348/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1656 - accuracy: 0.7651 - val_loss: 0.2421 - val_accuracy: 0.6534\n",
            "Epoch 349/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1653 - accuracy: 0.7675 - val_loss: 0.2431 - val_accuracy: 0.6390\n",
            "Epoch 350/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.7747 - val_loss: 0.2393 - val_accuracy: 0.6498\n",
            "Epoch 351/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1668 - accuracy: 0.7675 - val_loss: 0.2386 - val_accuracy: 0.6462\n",
            "Epoch 352/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1669 - accuracy: 0.7675 - val_loss: 0.2409 - val_accuracy: 0.6245\n",
            "Epoch 353/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1661 - accuracy: 0.7711 - val_loss: 0.2417 - val_accuracy: 0.6318\n",
            "Epoch 354/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1719 - accuracy: 0.7470 - val_loss: 0.2447 - val_accuracy: 0.6282\n",
            "Epoch 355/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1665 - accuracy: 0.7759 - val_loss: 0.2399 - val_accuracy: 0.6390\n",
            "Epoch 356/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1652 - accuracy: 0.7663 - val_loss: 0.2410 - val_accuracy: 0.6426\n",
            "Epoch 357/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1656 - accuracy: 0.7687 - val_loss: 0.2437 - val_accuracy: 0.6354\n",
            "Epoch 358/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1636 - accuracy: 0.7711 - val_loss: 0.2465 - val_accuracy: 0.6354\n",
            "Epoch 359/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1667 - accuracy: 0.7651 - val_loss: 0.2437 - val_accuracy: 0.6426\n",
            "Epoch 360/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.7663 - val_loss: 0.2431 - val_accuracy: 0.6534\n",
            "Epoch 361/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.2007 - accuracy: 0.7157 - val_loss: 0.2423 - val_accuracy: 0.6390\n",
            "Epoch 362/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1766 - accuracy: 0.7482 - val_loss: 0.2400 - val_accuracy: 0.6462\n",
            "Epoch 363/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1699 - accuracy: 0.7542 - val_loss: 0.2407 - val_accuracy: 0.6498\n",
            "Epoch 364/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.7590 - val_loss: 0.2417 - val_accuracy: 0.6462\n",
            "Epoch 365/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1659 - accuracy: 0.7602 - val_loss: 0.2437 - val_accuracy: 0.6354\n",
            "Epoch 366/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.7699 - val_loss: 0.2424 - val_accuracy: 0.6318\n",
            "Epoch 367/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1678 - accuracy: 0.7675 - val_loss: 0.2481 - val_accuracy: 0.6282\n",
            "Epoch 368/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1671 - accuracy: 0.7675 - val_loss: 0.2408 - val_accuracy: 0.6426\n",
            "Epoch 369/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1710 - accuracy: 0.7530 - val_loss: 0.2466 - val_accuracy: 0.6318\n",
            "Epoch 370/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1692 - accuracy: 0.7614 - val_loss: 0.2496 - val_accuracy: 0.6282\n",
            "Epoch 371/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1654 - accuracy: 0.7735 - val_loss: 0.2448 - val_accuracy: 0.6462\n",
            "Epoch 372/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1648 - accuracy: 0.7687 - val_loss: 0.2423 - val_accuracy: 0.6462\n",
            "Epoch 373/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1683 - accuracy: 0.7590 - val_loss: 0.2466 - val_accuracy: 0.6209\n",
            "Epoch 374/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1675 - accuracy: 0.7675 - val_loss: 0.2448 - val_accuracy: 0.6462\n",
            "Epoch 375/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1650 - accuracy: 0.7675 - val_loss: 0.2438 - val_accuracy: 0.6390\n",
            "Epoch 376/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1642 - accuracy: 0.7699 - val_loss: 0.2453 - val_accuracy: 0.6209\n",
            "Epoch 377/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1643 - accuracy: 0.7735 - val_loss: 0.2410 - val_accuracy: 0.6426\n",
            "Epoch 378/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.7542 - val_loss: 0.2520 - val_accuracy: 0.6137\n",
            "Epoch 379/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1706 - accuracy: 0.7554 - val_loss: 0.2414 - val_accuracy: 0.6354\n",
            "Epoch 380/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1659 - accuracy: 0.7663 - val_loss: 0.2413 - val_accuracy: 0.6390\n",
            "Epoch 381/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1680 - accuracy: 0.7651 - val_loss: 0.2429 - val_accuracy: 0.6354\n",
            "Epoch 382/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1677 - accuracy: 0.7675 - val_loss: 0.2458 - val_accuracy: 0.6173\n",
            "Epoch 383/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1639 - accuracy: 0.7651 - val_loss: 0.2419 - val_accuracy: 0.6354\n",
            "Epoch 384/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.7651 - val_loss: 0.2418 - val_accuracy: 0.6606\n",
            "Epoch 385/900\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.7699 - val_loss: 0.2486 - val_accuracy: 0.6209\n",
            "Epoch 386/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1634 - accuracy: 0.7687 - val_loss: 0.2474 - val_accuracy: 0.6282\n",
            "Epoch 387/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1662 - accuracy: 0.7663 - val_loss: 0.2409 - val_accuracy: 0.6498\n",
            "Epoch 388/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1650 - accuracy: 0.7614 - val_loss: 0.2423 - val_accuracy: 0.6426\n",
            "Epoch 389/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1626 - accuracy: 0.7687 - val_loss: 0.2437 - val_accuracy: 0.6318\n",
            "Epoch 390/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1632 - accuracy: 0.7735 - val_loss: 0.2434 - val_accuracy: 0.6354\n",
            "Epoch 391/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1643 - accuracy: 0.7663 - val_loss: 0.2417 - val_accuracy: 0.6426\n",
            "Epoch 392/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1637 - accuracy: 0.7663 - val_loss: 0.2457 - val_accuracy: 0.6354\n",
            "Epoch 393/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1626 - accuracy: 0.7771 - val_loss: 0.2432 - val_accuracy: 0.6426\n",
            "Epoch 394/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1659 - accuracy: 0.7675 - val_loss: 0.2427 - val_accuracy: 0.6534\n",
            "Epoch 395/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1664 - accuracy: 0.7614 - val_loss: 0.2434 - val_accuracy: 0.6390\n",
            "Epoch 396/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1648 - accuracy: 0.7687 - val_loss: 0.2411 - val_accuracy: 0.6498\n",
            "Epoch 397/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1705 - accuracy: 0.7530 - val_loss: 0.2501 - val_accuracy: 0.6137\n",
            "Epoch 398/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.7506 - val_loss: 0.2430 - val_accuracy: 0.6426\n",
            "Epoch 399/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1675 - accuracy: 0.7675 - val_loss: 0.2448 - val_accuracy: 0.6354\n",
            "Epoch 400/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1665 - accuracy: 0.7530 - val_loss: 0.2438 - val_accuracy: 0.6534\n",
            "Epoch 401/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1688 - accuracy: 0.7590 - val_loss: 0.2430 - val_accuracy: 0.6534\n",
            "Epoch 402/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.7651 - val_loss: 0.2458 - val_accuracy: 0.6390\n",
            "Epoch 403/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1649 - accuracy: 0.7614 - val_loss: 0.2460 - val_accuracy: 0.6318\n",
            "Epoch 404/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1652 - accuracy: 0.7663 - val_loss: 0.2455 - val_accuracy: 0.6390\n",
            "Epoch 405/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1628 - accuracy: 0.7711 - val_loss: 0.2451 - val_accuracy: 0.6282\n",
            "Epoch 406/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1641 - accuracy: 0.7747 - val_loss: 0.2468 - val_accuracy: 0.6318\n",
            "Epoch 407/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1656 - accuracy: 0.7675 - val_loss: 0.2501 - val_accuracy: 0.6137\n",
            "Epoch 408/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.7627 - val_loss: 0.2438 - val_accuracy: 0.6534\n",
            "Epoch 409/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1650 - accuracy: 0.7711 - val_loss: 0.2416 - val_accuracy: 0.6426\n",
            "Epoch 410/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1702 - accuracy: 0.7566 - val_loss: 0.2462 - val_accuracy: 0.6245\n",
            "Epoch 411/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1969 - accuracy: 0.7217 - val_loss: 0.2480 - val_accuracy: 0.6318\n",
            "Epoch 412/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1995 - accuracy: 0.7012 - val_loss: 0.2425 - val_accuracy: 0.6282\n",
            "Epoch 413/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1857 - accuracy: 0.7289 - val_loss: 0.2502 - val_accuracy: 0.6173\n",
            "Epoch 414/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1710 - accuracy: 0.7518 - val_loss: 0.2447 - val_accuracy: 0.6173\n",
            "Epoch 415/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.7735 - val_loss: 0.2455 - val_accuracy: 0.6245\n",
            "Epoch 416/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1633 - accuracy: 0.7783 - val_loss: 0.2413 - val_accuracy: 0.6282\n",
            "Epoch 417/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1650 - accuracy: 0.7614 - val_loss: 0.2429 - val_accuracy: 0.6390\n",
            "Epoch 418/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.7759 - val_loss: 0.2416 - val_accuracy: 0.6426\n",
            "Epoch 419/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1641 - accuracy: 0.7627 - val_loss: 0.2438 - val_accuracy: 0.6354\n",
            "Epoch 420/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1660 - accuracy: 0.7699 - val_loss: 0.2450 - val_accuracy: 0.6354\n",
            "Epoch 421/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.7747 - val_loss: 0.2444 - val_accuracy: 0.6282\n",
            "Epoch 422/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1627 - accuracy: 0.7771 - val_loss: 0.2413 - val_accuracy: 0.6426\n",
            "Epoch 423/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1615 - accuracy: 0.7759 - val_loss: 0.2454 - val_accuracy: 0.6282\n",
            "Epoch 424/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.7819 - val_loss: 0.2422 - val_accuracy: 0.6282\n",
            "Epoch 425/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.7747 - val_loss: 0.2433 - val_accuracy: 0.6318\n",
            "Epoch 426/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1638 - accuracy: 0.7590 - val_loss: 0.2434 - val_accuracy: 0.6318\n",
            "Epoch 427/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.7723 - val_loss: 0.2461 - val_accuracy: 0.6282\n",
            "Epoch 428/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.7687 - val_loss: 0.2430 - val_accuracy: 0.6354\n",
            "Epoch 429/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1637 - accuracy: 0.7759 - val_loss: 0.2424 - val_accuracy: 0.6390\n",
            "Epoch 430/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1652 - accuracy: 0.7627 - val_loss: 0.2453 - val_accuracy: 0.6282\n",
            "Epoch 431/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1619 - accuracy: 0.7747 - val_loss: 0.2429 - val_accuracy: 0.6318\n",
            "Epoch 432/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1664 - accuracy: 0.7614 - val_loss: 0.2462 - val_accuracy: 0.6282\n",
            "Epoch 433/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1676 - accuracy: 0.7639 - val_loss: 0.2458 - val_accuracy: 0.6354\n",
            "Epoch 434/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1719 - accuracy: 0.7578 - val_loss: 0.2458 - val_accuracy: 0.6390\n",
            "Epoch 435/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1642 - accuracy: 0.7735 - val_loss: 0.2407 - val_accuracy: 0.6606\n",
            "Epoch 436/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1634 - accuracy: 0.7723 - val_loss: 0.2420 - val_accuracy: 0.6462\n",
            "Epoch 437/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1643 - accuracy: 0.7614 - val_loss: 0.2442 - val_accuracy: 0.6245\n",
            "Epoch 438/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1632 - accuracy: 0.7699 - val_loss: 0.2477 - val_accuracy: 0.6390\n",
            "Epoch 439/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1619 - accuracy: 0.7759 - val_loss: 0.2440 - val_accuracy: 0.6354\n",
            "Epoch 440/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1654 - accuracy: 0.7651 - val_loss: 0.2434 - val_accuracy: 0.6282\n",
            "Epoch 441/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1610 - accuracy: 0.7771 - val_loss: 0.2422 - val_accuracy: 0.6282\n",
            "Epoch 442/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1637 - accuracy: 0.7699 - val_loss: 0.2476 - val_accuracy: 0.6209\n",
            "Epoch 443/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.7554 - val_loss: 0.2463 - val_accuracy: 0.6390\n",
            "Epoch 444/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1692 - accuracy: 0.7578 - val_loss: 0.2412 - val_accuracy: 0.6462\n",
            "Epoch 445/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1655 - accuracy: 0.7675 - val_loss: 0.2467 - val_accuracy: 0.6318\n",
            "Epoch 446/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1656 - accuracy: 0.7639 - val_loss: 0.2467 - val_accuracy: 0.6245\n",
            "Epoch 447/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1775 - accuracy: 0.7470 - val_loss: 0.2399 - val_accuracy: 0.6354\n",
            "Epoch 448/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1675 - accuracy: 0.7530 - val_loss: 0.2512 - val_accuracy: 0.6245\n",
            "Epoch 449/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1652 - accuracy: 0.7735 - val_loss: 0.2451 - val_accuracy: 0.6426\n",
            "Epoch 450/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1623 - accuracy: 0.7627 - val_loss: 0.2461 - val_accuracy: 0.6282\n",
            "Epoch 451/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1620 - accuracy: 0.7747 - val_loss: 0.2463 - val_accuracy: 0.6282\n",
            "Epoch 452/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1630 - accuracy: 0.7711 - val_loss: 0.2476 - val_accuracy: 0.6354\n",
            "Epoch 453/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1613 - accuracy: 0.7687 - val_loss: 0.2447 - val_accuracy: 0.6498\n",
            "Epoch 454/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1648 - accuracy: 0.7627 - val_loss: 0.2450 - val_accuracy: 0.6245\n",
            "Epoch 455/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1635 - accuracy: 0.7675 - val_loss: 0.2513 - val_accuracy: 0.6209\n",
            "Epoch 456/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1649 - accuracy: 0.7602 - val_loss: 0.2456 - val_accuracy: 0.6245\n",
            "Epoch 457/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1626 - accuracy: 0.7747 - val_loss: 0.2469 - val_accuracy: 0.6282\n",
            "Epoch 458/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1656 - accuracy: 0.7663 - val_loss: 0.2432 - val_accuracy: 0.6354\n",
            "Epoch 459/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1646 - accuracy: 0.7663 - val_loss: 0.2414 - val_accuracy: 0.6498\n",
            "Epoch 460/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1677 - accuracy: 0.7651 - val_loss: 0.2426 - val_accuracy: 0.6318\n",
            "Epoch 461/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1630 - accuracy: 0.7711 - val_loss: 0.2509 - val_accuracy: 0.6137\n",
            "Epoch 462/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1630 - accuracy: 0.7699 - val_loss: 0.2523 - val_accuracy: 0.6245\n",
            "Epoch 463/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1644 - accuracy: 0.7723 - val_loss: 0.2427 - val_accuracy: 0.6390\n",
            "Epoch 464/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1638 - accuracy: 0.7651 - val_loss: 0.2441 - val_accuracy: 0.6318\n",
            "Epoch 465/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1630 - accuracy: 0.7627 - val_loss: 0.2511 - val_accuracy: 0.6282\n",
            "Epoch 466/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1623 - accuracy: 0.7711 - val_loss: 0.2452 - val_accuracy: 0.6318\n",
            "Epoch 467/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1612 - accuracy: 0.7699 - val_loss: 0.2503 - val_accuracy: 0.6282\n",
            "Epoch 468/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1631 - accuracy: 0.7711 - val_loss: 0.2437 - val_accuracy: 0.6462\n",
            "Epoch 469/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1616 - accuracy: 0.7735 - val_loss: 0.2482 - val_accuracy: 0.6282\n",
            "Epoch 470/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1642 - accuracy: 0.7627 - val_loss: 0.2468 - val_accuracy: 0.6209\n",
            "Epoch 471/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1616 - accuracy: 0.7699 - val_loss: 0.2494 - val_accuracy: 0.6209\n",
            "Epoch 472/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1605 - accuracy: 0.7795 - val_loss: 0.2460 - val_accuracy: 0.6209\n",
            "Epoch 473/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1612 - accuracy: 0.7759 - val_loss: 0.2492 - val_accuracy: 0.6209\n",
            "Epoch 474/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1602 - accuracy: 0.7867 - val_loss: 0.2466 - val_accuracy: 0.6354\n",
            "Epoch 475/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.7554 - val_loss: 0.2430 - val_accuracy: 0.6354\n",
            "Epoch 476/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1720 - accuracy: 0.7566 - val_loss: 0.2408 - val_accuracy: 0.6498\n",
            "Epoch 477/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1653 - accuracy: 0.7554 - val_loss: 0.2519 - val_accuracy: 0.6209\n",
            "Epoch 478/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1663 - accuracy: 0.7663 - val_loss: 0.2425 - val_accuracy: 0.6354\n",
            "Epoch 479/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1629 - accuracy: 0.7675 - val_loss: 0.2423 - val_accuracy: 0.6426\n",
            "Epoch 480/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1603 - accuracy: 0.7807 - val_loss: 0.2424 - val_accuracy: 0.6462\n",
            "Epoch 481/900\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.7410 - val_loss: 0.2530 - val_accuracy: 0.6137\n",
            "Epoch 482/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.7578 - val_loss: 0.2404 - val_accuracy: 0.6390\n",
            "Epoch 483/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.7771 - val_loss: 0.2429 - val_accuracy: 0.6354\n",
            "Epoch 484/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.7783 - val_loss: 0.2451 - val_accuracy: 0.6354\n",
            "Epoch 485/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1613 - accuracy: 0.7747 - val_loss: 0.2389 - val_accuracy: 0.6462\n",
            "Epoch 486/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1620 - accuracy: 0.7699 - val_loss: 0.2470 - val_accuracy: 0.6318\n",
            "Epoch 487/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.7711 - val_loss: 0.2408 - val_accuracy: 0.6426\n",
            "Epoch 488/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1621 - accuracy: 0.7747 - val_loss: 0.2413 - val_accuracy: 0.6426\n",
            "Epoch 489/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.7747 - val_loss: 0.2419 - val_accuracy: 0.6498\n",
            "Epoch 490/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1642 - accuracy: 0.7627 - val_loss: 0.2544 - val_accuracy: 0.6173\n",
            "Epoch 491/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1647 - accuracy: 0.7687 - val_loss: 0.2439 - val_accuracy: 0.6318\n",
            "Epoch 492/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1604 - accuracy: 0.7699 - val_loss: 0.2462 - val_accuracy: 0.6390\n",
            "Epoch 493/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1608 - accuracy: 0.7831 - val_loss: 0.2424 - val_accuracy: 0.6462\n",
            "Epoch 494/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1601 - accuracy: 0.7675 - val_loss: 0.2424 - val_accuracy: 0.6498\n",
            "Epoch 495/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.7723 - val_loss: 0.2469 - val_accuracy: 0.6209\n",
            "Epoch 496/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1607 - accuracy: 0.7819 - val_loss: 0.2463 - val_accuracy: 0.6318\n",
            "Epoch 497/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1607 - accuracy: 0.7759 - val_loss: 0.2453 - val_accuracy: 0.6137\n",
            "Epoch 498/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.7699 - val_loss: 0.2465 - val_accuracy: 0.6209\n",
            "Epoch 499/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1636 - accuracy: 0.7747 - val_loss: 0.2449 - val_accuracy: 0.6498\n",
            "Epoch 500/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.7783 - val_loss: 0.2468 - val_accuracy: 0.6354\n",
            "Epoch 501/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1668 - accuracy: 0.7663 - val_loss: 0.2451 - val_accuracy: 0.6318\n",
            "Epoch 502/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1650 - accuracy: 0.7639 - val_loss: 0.2473 - val_accuracy: 0.6282\n",
            "Epoch 503/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.7795 - val_loss: 0.2463 - val_accuracy: 0.6318\n",
            "Epoch 504/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1619 - accuracy: 0.7699 - val_loss: 0.2543 - val_accuracy: 0.6065\n",
            "Epoch 505/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.7747 - val_loss: 0.2442 - val_accuracy: 0.6462\n",
            "Epoch 506/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.7795 - val_loss: 0.2446 - val_accuracy: 0.6354\n",
            "Epoch 507/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1627 - accuracy: 0.7627 - val_loss: 0.2462 - val_accuracy: 0.6173\n",
            "Epoch 508/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1618 - accuracy: 0.7759 - val_loss: 0.2450 - val_accuracy: 0.6318\n",
            "Epoch 509/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1608 - accuracy: 0.7735 - val_loss: 0.2433 - val_accuracy: 0.6390\n",
            "Epoch 510/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1634 - accuracy: 0.7735 - val_loss: 0.2455 - val_accuracy: 0.6173\n",
            "Epoch 511/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1621 - accuracy: 0.7747 - val_loss: 0.2438 - val_accuracy: 0.6318\n",
            "Epoch 512/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1598 - accuracy: 0.7711 - val_loss: 0.2463 - val_accuracy: 0.6245\n",
            "Epoch 513/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1601 - accuracy: 0.7807 - val_loss: 0.2479 - val_accuracy: 0.6137\n",
            "Epoch 514/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1721 - accuracy: 0.7494 - val_loss: 0.2510 - val_accuracy: 0.6209\n",
            "Epoch 515/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1674 - accuracy: 0.7614 - val_loss: 0.2422 - val_accuracy: 0.6282\n",
            "Epoch 516/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1616 - accuracy: 0.7723 - val_loss: 0.2528 - val_accuracy: 0.6173\n",
            "Epoch 517/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.7807 - val_loss: 0.2448 - val_accuracy: 0.6354\n",
            "Epoch 518/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1617 - accuracy: 0.7675 - val_loss: 0.2485 - val_accuracy: 0.6209\n",
            "Epoch 519/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.7699 - val_loss: 0.2503 - val_accuracy: 0.6173\n",
            "Epoch 520/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1657 - accuracy: 0.7675 - val_loss: 0.2451 - val_accuracy: 0.6426\n",
            "Epoch 521/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1633 - accuracy: 0.7735 - val_loss: 0.2455 - val_accuracy: 0.6245\n",
            "Epoch 522/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1663 - accuracy: 0.7566 - val_loss: 0.2468 - val_accuracy: 0.6318\n",
            "Epoch 523/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.7663 - val_loss: 0.2423 - val_accuracy: 0.6498\n",
            "Epoch 524/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1614 - accuracy: 0.7711 - val_loss: 0.2485 - val_accuracy: 0.6245\n",
            "Epoch 525/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.7783 - val_loss: 0.2431 - val_accuracy: 0.6462\n",
            "Epoch 526/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1633 - accuracy: 0.7663 - val_loss: 0.2521 - val_accuracy: 0.6137\n",
            "Epoch 527/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1609 - accuracy: 0.7831 - val_loss: 0.2465 - val_accuracy: 0.6209\n",
            "Epoch 528/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1603 - accuracy: 0.7819 - val_loss: 0.2430 - val_accuracy: 0.6318\n",
            "Epoch 529/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1615 - accuracy: 0.7759 - val_loss: 0.2497 - val_accuracy: 0.6173\n",
            "Epoch 530/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1639 - accuracy: 0.7675 - val_loss: 0.2447 - val_accuracy: 0.6282\n",
            "Epoch 531/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1604 - accuracy: 0.7663 - val_loss: 0.2448 - val_accuracy: 0.6354\n",
            "Epoch 532/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.7614 - val_loss: 0.2429 - val_accuracy: 0.6354\n",
            "Epoch 533/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1631 - accuracy: 0.7711 - val_loss: 0.2470 - val_accuracy: 0.6282\n",
            "Epoch 534/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1590 - accuracy: 0.7807 - val_loss: 0.2454 - val_accuracy: 0.6426\n",
            "Epoch 535/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1608 - accuracy: 0.7795 - val_loss: 0.2441 - val_accuracy: 0.6426\n",
            "Epoch 536/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1659 - accuracy: 0.7651 - val_loss: 0.2466 - val_accuracy: 0.6318\n",
            "Epoch 537/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1620 - accuracy: 0.7747 - val_loss: 0.2457 - val_accuracy: 0.6354\n",
            "Epoch 538/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1601 - accuracy: 0.7699 - val_loss: 0.2562 - val_accuracy: 0.6101\n",
            "Epoch 539/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1635 - accuracy: 0.7687 - val_loss: 0.2496 - val_accuracy: 0.6209\n",
            "Epoch 540/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1590 - accuracy: 0.7783 - val_loss: 0.2450 - val_accuracy: 0.6390\n",
            "Epoch 541/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.7711 - val_loss: 0.2501 - val_accuracy: 0.6173\n",
            "Epoch 542/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1590 - accuracy: 0.7795 - val_loss: 0.2467 - val_accuracy: 0.6354\n",
            "Epoch 543/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1646 - accuracy: 0.7663 - val_loss: 0.2463 - val_accuracy: 0.6245\n",
            "Epoch 544/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1645 - accuracy: 0.7651 - val_loss: 0.2417 - val_accuracy: 0.6282\n",
            "Epoch 545/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1599 - accuracy: 0.7819 - val_loss: 0.2452 - val_accuracy: 0.6390\n",
            "Epoch 546/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.7759 - val_loss: 0.2463 - val_accuracy: 0.6354\n",
            "Epoch 547/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1655 - accuracy: 0.7663 - val_loss: 0.2551 - val_accuracy: 0.6065\n",
            "Epoch 548/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1642 - accuracy: 0.7675 - val_loss: 0.2588 - val_accuracy: 0.6065\n",
            "Epoch 549/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1803 - accuracy: 0.7253 - val_loss: 0.2457 - val_accuracy: 0.6354\n",
            "Epoch 550/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.7675 - val_loss: 0.2434 - val_accuracy: 0.6426\n",
            "Epoch 551/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1609 - accuracy: 0.7699 - val_loss: 0.2513 - val_accuracy: 0.6209\n",
            "Epoch 552/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.7759 - val_loss: 0.2420 - val_accuracy: 0.6318\n",
            "Epoch 553/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1583 - accuracy: 0.7759 - val_loss: 0.2435 - val_accuracy: 0.6426\n",
            "Epoch 554/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1587 - accuracy: 0.7795 - val_loss: 0.2533 - val_accuracy: 0.6282\n",
            "Epoch 555/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1590 - accuracy: 0.7819 - val_loss: 0.2498 - val_accuracy: 0.6245\n",
            "Epoch 556/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.7867 - val_loss: 0.2487 - val_accuracy: 0.6282\n",
            "Epoch 557/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1588 - accuracy: 0.7759 - val_loss: 0.2454 - val_accuracy: 0.6282\n",
            "Epoch 558/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1608 - accuracy: 0.7735 - val_loss: 0.2487 - val_accuracy: 0.6318\n",
            "Epoch 559/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.7795 - val_loss: 0.2477 - val_accuracy: 0.6282\n",
            "Epoch 560/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1568 - accuracy: 0.7843 - val_loss: 0.2469 - val_accuracy: 0.6354\n",
            "Epoch 561/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1629 - accuracy: 0.7675 - val_loss: 0.2469 - val_accuracy: 0.6282\n",
            "Epoch 562/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1628 - accuracy: 0.7687 - val_loss: 0.2497 - val_accuracy: 0.6318\n",
            "Epoch 563/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.7819 - val_loss: 0.2461 - val_accuracy: 0.6318\n",
            "Epoch 564/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1582 - accuracy: 0.7807 - val_loss: 0.2489 - val_accuracy: 0.6282\n",
            "Epoch 565/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1589 - accuracy: 0.7783 - val_loss: 0.2475 - val_accuracy: 0.6173\n",
            "Epoch 566/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1570 - accuracy: 0.7867 - val_loss: 0.2458 - val_accuracy: 0.6390\n",
            "Epoch 567/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1595 - accuracy: 0.7723 - val_loss: 0.2482 - val_accuracy: 0.6354\n",
            "Epoch 568/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1602 - accuracy: 0.7711 - val_loss: 0.2409 - val_accuracy: 0.6354\n",
            "Epoch 569/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1602 - accuracy: 0.7711 - val_loss: 0.2444 - val_accuracy: 0.6282\n",
            "Epoch 570/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.7675 - val_loss: 0.2487 - val_accuracy: 0.6282\n",
            "Epoch 571/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 0.7795 - val_loss: 0.2442 - val_accuracy: 0.6318\n",
            "Epoch 572/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1592 - accuracy: 0.7711 - val_loss: 0.2460 - val_accuracy: 0.6137\n",
            "Epoch 573/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1608 - accuracy: 0.7747 - val_loss: 0.2528 - val_accuracy: 0.6173\n",
            "Epoch 574/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.7783 - val_loss: 0.2482 - val_accuracy: 0.6245\n",
            "Epoch 575/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1578 - accuracy: 0.7831 - val_loss: 0.2468 - val_accuracy: 0.6173\n",
            "Epoch 576/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.7807 - val_loss: 0.2516 - val_accuracy: 0.6173\n",
            "Epoch 577/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1600 - accuracy: 0.7759 - val_loss: 0.2455 - val_accuracy: 0.6318\n",
            "Epoch 578/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1582 - accuracy: 0.7771 - val_loss: 0.2480 - val_accuracy: 0.6318\n",
            "Epoch 579/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1583 - accuracy: 0.7759 - val_loss: 0.2476 - val_accuracy: 0.6245\n",
            "Epoch 580/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1585 - accuracy: 0.7783 - val_loss: 0.2512 - val_accuracy: 0.6209\n",
            "Epoch 581/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1582 - accuracy: 0.7807 - val_loss: 0.2463 - val_accuracy: 0.6318\n",
            "Epoch 582/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1577 - accuracy: 0.7807 - val_loss: 0.2502 - val_accuracy: 0.6101\n",
            "Epoch 583/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1584 - accuracy: 0.7783 - val_loss: 0.2460 - val_accuracy: 0.6173\n",
            "Epoch 584/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1615 - accuracy: 0.7807 - val_loss: 0.2506 - val_accuracy: 0.6318\n",
            "Epoch 585/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1639 - accuracy: 0.7663 - val_loss: 0.2464 - val_accuracy: 0.6462\n",
            "Epoch 586/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1593 - accuracy: 0.7855 - val_loss: 0.2490 - val_accuracy: 0.6390\n",
            "Epoch 587/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1570 - accuracy: 0.7843 - val_loss: 0.2523 - val_accuracy: 0.6245\n",
            "Epoch 588/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1591 - accuracy: 0.7807 - val_loss: 0.2510 - val_accuracy: 0.6101\n",
            "Epoch 589/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1601 - accuracy: 0.7699 - val_loss: 0.2551 - val_accuracy: 0.6209\n",
            "Epoch 590/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1715 - accuracy: 0.7494 - val_loss: 0.2506 - val_accuracy: 0.6245\n",
            "Epoch 591/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1688 - accuracy: 0.7651 - val_loss: 0.2470 - val_accuracy: 0.6137\n",
            "Epoch 592/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1580 - accuracy: 0.7771 - val_loss: 0.2534 - val_accuracy: 0.6137\n",
            "Epoch 593/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1578 - accuracy: 0.7831 - val_loss: 0.2460 - val_accuracy: 0.6209\n",
            "Epoch 594/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1610 - accuracy: 0.7711 - val_loss: 0.2527 - val_accuracy: 0.6173\n",
            "Epoch 595/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1575 - accuracy: 0.7819 - val_loss: 0.2507 - val_accuracy: 0.6173\n",
            "Epoch 596/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1590 - accuracy: 0.7855 - val_loss: 0.2464 - val_accuracy: 0.6209\n",
            "Epoch 597/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1645 - accuracy: 0.7639 - val_loss: 0.2454 - val_accuracy: 0.6173\n",
            "Epoch 598/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1590 - accuracy: 0.7783 - val_loss: 0.2487 - val_accuracy: 0.6209\n",
            "Epoch 599/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1606 - accuracy: 0.7771 - val_loss: 0.2499 - val_accuracy: 0.6029\n",
            "Epoch 600/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1615 - accuracy: 0.7735 - val_loss: 0.2506 - val_accuracy: 0.6209\n",
            "Epoch 601/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1636 - accuracy: 0.7651 - val_loss: 0.2455 - val_accuracy: 0.6426\n",
            "Epoch 602/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1559 - accuracy: 0.7916 - val_loss: 0.2465 - val_accuracy: 0.6426\n",
            "Epoch 603/900\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1571 - accuracy: 0.7855 - val_loss: 0.2501 - val_accuracy: 0.6245\n",
            "Epoch 604/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1555 - accuracy: 0.7880 - val_loss: 0.2552 - val_accuracy: 0.6245\n",
            "Epoch 605/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1627 - accuracy: 0.7723 - val_loss: 0.2461 - val_accuracy: 0.6354\n",
            "Epoch 606/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1565 - accuracy: 0.7843 - val_loss: 0.2478 - val_accuracy: 0.6354\n",
            "Epoch 607/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1560 - accuracy: 0.7867 - val_loss: 0.2511 - val_accuracy: 0.6282\n",
            "Epoch 608/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1619 - accuracy: 0.7759 - val_loss: 0.2514 - val_accuracy: 0.6137\n",
            "Epoch 609/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1562 - accuracy: 0.7867 - val_loss: 0.2524 - val_accuracy: 0.6137\n",
            "Epoch 610/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1587 - accuracy: 0.7867 - val_loss: 0.2464 - val_accuracy: 0.6354\n",
            "Epoch 611/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1563 - accuracy: 0.7831 - val_loss: 0.2474 - val_accuracy: 0.6318\n",
            "Epoch 612/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1561 - accuracy: 0.7928 - val_loss: 0.2521 - val_accuracy: 0.6282\n",
            "Epoch 613/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1558 - accuracy: 0.7855 - val_loss: 0.2511 - val_accuracy: 0.6245\n",
            "Epoch 614/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1567 - accuracy: 0.7904 - val_loss: 0.2496 - val_accuracy: 0.6245\n",
            "Epoch 615/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1568 - accuracy: 0.7807 - val_loss: 0.2502 - val_accuracy: 0.6245\n",
            "Epoch 616/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1586 - accuracy: 0.7807 - val_loss: 0.2498 - val_accuracy: 0.6245\n",
            "Epoch 617/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1601 - accuracy: 0.7771 - val_loss: 0.2486 - val_accuracy: 0.6282\n",
            "Epoch 618/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1594 - accuracy: 0.7711 - val_loss: 0.2475 - val_accuracy: 0.6282\n",
            "Epoch 619/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1583 - accuracy: 0.7759 - val_loss: 0.2453 - val_accuracy: 0.6354\n",
            "Epoch 620/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1576 - accuracy: 0.7807 - val_loss: 0.2480 - val_accuracy: 0.6209\n",
            "Epoch 621/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.7771 - val_loss: 0.2470 - val_accuracy: 0.6354\n",
            "Epoch 622/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1550 - accuracy: 0.7867 - val_loss: 0.2536 - val_accuracy: 0.6137\n",
            "Epoch 623/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.7843 - val_loss: 0.2568 - val_accuracy: 0.6209\n",
            "Epoch 624/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1589 - accuracy: 0.7843 - val_loss: 0.2510 - val_accuracy: 0.6245\n",
            "Epoch 625/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.7904 - val_loss: 0.2487 - val_accuracy: 0.6282\n",
            "Epoch 626/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1566 - accuracy: 0.7783 - val_loss: 0.2490 - val_accuracy: 0.6245\n",
            "Epoch 627/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1568 - accuracy: 0.7795 - val_loss: 0.2490 - val_accuracy: 0.6354\n",
            "Epoch 628/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1607 - accuracy: 0.7747 - val_loss: 0.2490 - val_accuracy: 0.6245\n",
            "Epoch 629/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1611 - accuracy: 0.7747 - val_loss: 0.2496 - val_accuracy: 0.6318\n",
            "Epoch 630/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1740 - accuracy: 0.7434 - val_loss: 0.2512 - val_accuracy: 0.6065\n",
            "Epoch 631/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.7723 - val_loss: 0.2414 - val_accuracy: 0.6245\n",
            "Epoch 632/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1594 - accuracy: 0.7747 - val_loss: 0.2516 - val_accuracy: 0.6101\n",
            "Epoch 633/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1698 - accuracy: 0.7506 - val_loss: 0.2493 - val_accuracy: 0.6245\n",
            "Epoch 634/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1602 - accuracy: 0.7699 - val_loss: 0.2459 - val_accuracy: 0.6354\n",
            "Epoch 635/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1584 - accuracy: 0.7795 - val_loss: 0.2434 - val_accuracy: 0.6318\n",
            "Epoch 636/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1649 - accuracy: 0.7627 - val_loss: 0.2435 - val_accuracy: 0.6426\n",
            "Epoch 637/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1599 - accuracy: 0.7711 - val_loss: 0.2417 - val_accuracy: 0.6173\n",
            "Epoch 638/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1631 - accuracy: 0.7675 - val_loss: 0.2512 - val_accuracy: 0.6282\n",
            "Epoch 639/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1597 - accuracy: 0.7699 - val_loss: 0.2446 - val_accuracy: 0.6282\n",
            "Epoch 640/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1585 - accuracy: 0.7880 - val_loss: 0.2508 - val_accuracy: 0.6282\n",
            "Epoch 641/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1595 - accuracy: 0.7771 - val_loss: 0.2458 - val_accuracy: 0.6245\n",
            "Epoch 642/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1546 - accuracy: 0.7819 - val_loss: 0.2513 - val_accuracy: 0.6065\n",
            "Epoch 643/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1590 - accuracy: 0.7807 - val_loss: 0.2525 - val_accuracy: 0.6029\n",
            "Epoch 644/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1811 - accuracy: 0.7361 - val_loss: 0.2494 - val_accuracy: 0.6173\n",
            "Epoch 645/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1634 - accuracy: 0.7663 - val_loss: 0.2515 - val_accuracy: 0.6173\n",
            "Epoch 646/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1572 - accuracy: 0.7831 - val_loss: 0.2529 - val_accuracy: 0.5993\n",
            "Epoch 647/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1603 - accuracy: 0.7675 - val_loss: 0.2462 - val_accuracy: 0.6137\n",
            "Epoch 648/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1591 - accuracy: 0.7807 - val_loss: 0.2495 - val_accuracy: 0.6101\n",
            "Epoch 649/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1545 - accuracy: 0.7831 - val_loss: 0.2469 - val_accuracy: 0.6245\n",
            "Epoch 650/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1545 - accuracy: 0.7843 - val_loss: 0.2463 - val_accuracy: 0.6173\n",
            "Epoch 651/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1585 - accuracy: 0.7819 - val_loss: 0.2444 - val_accuracy: 0.6245\n",
            "Epoch 652/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1559 - accuracy: 0.7855 - val_loss: 0.2483 - val_accuracy: 0.6282\n",
            "Epoch 653/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1558 - accuracy: 0.7867 - val_loss: 0.2481 - val_accuracy: 0.6209\n",
            "Epoch 654/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1553 - accuracy: 0.7940 - val_loss: 0.2472 - val_accuracy: 0.6209\n",
            "Epoch 655/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1563 - accuracy: 0.7855 - val_loss: 0.2499 - val_accuracy: 0.6173\n",
            "Epoch 656/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.7855 - val_loss: 0.2484 - val_accuracy: 0.6173\n",
            "Epoch 657/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.7904 - val_loss: 0.2508 - val_accuracy: 0.6137\n",
            "Epoch 658/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1553 - accuracy: 0.7904 - val_loss: 0.2462 - val_accuracy: 0.6245\n",
            "Epoch 659/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1583 - accuracy: 0.7916 - val_loss: 0.2522 - val_accuracy: 0.6245\n",
            "Epoch 660/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.7807 - val_loss: 0.2498 - val_accuracy: 0.6245\n",
            "Epoch 661/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1555 - accuracy: 0.7795 - val_loss: 0.2521 - val_accuracy: 0.6173\n",
            "Epoch 662/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1542 - accuracy: 0.7928 - val_loss: 0.2496 - val_accuracy: 0.6137\n",
            "Epoch 663/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1653 - accuracy: 0.7663 - val_loss: 0.2540 - val_accuracy: 0.6065\n",
            "Epoch 664/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1673 - accuracy: 0.7675 - val_loss: 0.2575 - val_accuracy: 0.5957\n",
            "Epoch 665/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1672 - accuracy: 0.7675 - val_loss: 0.2439 - val_accuracy: 0.6245\n",
            "Epoch 666/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1572 - accuracy: 0.7855 - val_loss: 0.2452 - val_accuracy: 0.6245\n",
            "Epoch 667/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1597 - accuracy: 0.7867 - val_loss: 0.2517 - val_accuracy: 0.6137\n",
            "Epoch 668/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1543 - accuracy: 0.7867 - val_loss: 0.2474 - val_accuracy: 0.6101\n",
            "Epoch 669/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1549 - accuracy: 0.7867 - val_loss: 0.2484 - val_accuracy: 0.6245\n",
            "Epoch 670/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1553 - accuracy: 0.7892 - val_loss: 0.2464 - val_accuracy: 0.6209\n",
            "Epoch 671/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1541 - accuracy: 0.7892 - val_loss: 0.2530 - val_accuracy: 0.6137\n",
            "Epoch 672/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.7916 - val_loss: 0.2514 - val_accuracy: 0.6245\n",
            "Epoch 673/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1585 - accuracy: 0.7831 - val_loss: 0.2482 - val_accuracy: 0.6173\n",
            "Epoch 674/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.7940 - val_loss: 0.2484 - val_accuracy: 0.6282\n",
            "Epoch 675/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1580 - accuracy: 0.7831 - val_loss: 0.2495 - val_accuracy: 0.6065\n",
            "Epoch 676/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1536 - accuracy: 0.7843 - val_loss: 0.2517 - val_accuracy: 0.6173\n",
            "Epoch 677/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.7928 - val_loss: 0.2454 - val_accuracy: 0.6354\n",
            "Epoch 678/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.7795 - val_loss: 0.2494 - val_accuracy: 0.6282\n",
            "Epoch 679/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1556 - accuracy: 0.7892 - val_loss: 0.2488 - val_accuracy: 0.6173\n",
            "Epoch 680/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1546 - accuracy: 0.7819 - val_loss: 0.2510 - val_accuracy: 0.6209\n",
            "Epoch 681/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1563 - accuracy: 0.7771 - val_loss: 0.2582 - val_accuracy: 0.6029\n",
            "Epoch 682/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1531 - accuracy: 0.7964 - val_loss: 0.2485 - val_accuracy: 0.6173\n",
            "Epoch 683/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1557 - accuracy: 0.7831 - val_loss: 0.2506 - val_accuracy: 0.6245\n",
            "Epoch 684/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.7843 - val_loss: 0.2531 - val_accuracy: 0.6209\n",
            "Epoch 685/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1545 - accuracy: 0.7855 - val_loss: 0.2516 - val_accuracy: 0.6101\n",
            "Epoch 686/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1533 - accuracy: 0.7892 - val_loss: 0.2494 - val_accuracy: 0.6137\n",
            "Epoch 687/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1542 - accuracy: 0.7976 - val_loss: 0.2469 - val_accuracy: 0.6245\n",
            "Epoch 688/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1611 - accuracy: 0.7807 - val_loss: 0.2491 - val_accuracy: 0.6354\n",
            "Epoch 689/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1567 - accuracy: 0.7783 - val_loss: 0.2489 - val_accuracy: 0.6029\n",
            "Epoch 690/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.7867 - val_loss: 0.2449 - val_accuracy: 0.6462\n",
            "Epoch 691/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.7699 - val_loss: 0.2452 - val_accuracy: 0.6173\n",
            "Epoch 692/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1566 - accuracy: 0.7771 - val_loss: 0.2519 - val_accuracy: 0.6209\n",
            "Epoch 693/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1564 - accuracy: 0.7880 - val_loss: 0.2490 - val_accuracy: 0.6137\n",
            "Epoch 694/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1566 - accuracy: 0.7807 - val_loss: 0.2477 - val_accuracy: 0.6245\n",
            "Epoch 695/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1581 - accuracy: 0.7843 - val_loss: 0.2497 - val_accuracy: 0.6065\n",
            "Epoch 696/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1537 - accuracy: 0.7892 - val_loss: 0.2467 - val_accuracy: 0.6173\n",
            "Epoch 697/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1560 - accuracy: 0.7675 - val_loss: 0.2556 - val_accuracy: 0.6173\n",
            "Epoch 698/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1682 - accuracy: 0.7675 - val_loss: 0.2533 - val_accuracy: 0.6173\n",
            "Epoch 699/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.7602 - val_loss: 0.2564 - val_accuracy: 0.6065\n",
            "Epoch 700/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1675 - accuracy: 0.7590 - val_loss: 0.2468 - val_accuracy: 0.6137\n",
            "Epoch 701/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1605 - accuracy: 0.7819 - val_loss: 0.2466 - val_accuracy: 0.6209\n",
            "Epoch 702/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1533 - accuracy: 0.7904 - val_loss: 0.2480 - val_accuracy: 0.6101\n",
            "Epoch 703/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1548 - accuracy: 0.7867 - val_loss: 0.2529 - val_accuracy: 0.6137\n",
            "Epoch 704/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1557 - accuracy: 0.7867 - val_loss: 0.2511 - val_accuracy: 0.6209\n",
            "Epoch 705/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1519 - accuracy: 0.7964 - val_loss: 0.2552 - val_accuracy: 0.6029\n",
            "Epoch 706/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.7807 - val_loss: 0.2504 - val_accuracy: 0.6101\n",
            "Epoch 707/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.7819 - val_loss: 0.2493 - val_accuracy: 0.6065\n",
            "Epoch 708/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1542 - accuracy: 0.7892 - val_loss: 0.2558 - val_accuracy: 0.6173\n",
            "Epoch 709/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1555 - accuracy: 0.7904 - val_loss: 0.2486 - val_accuracy: 0.6245\n",
            "Epoch 710/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1543 - accuracy: 0.7783 - val_loss: 0.2526 - val_accuracy: 0.6137\n",
            "Epoch 711/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1528 - accuracy: 0.7892 - val_loss: 0.2548 - val_accuracy: 0.6101\n",
            "Epoch 712/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.7904 - val_loss: 0.2598 - val_accuracy: 0.5957\n",
            "Epoch 713/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1571 - accuracy: 0.7807 - val_loss: 0.2520 - val_accuracy: 0.6137\n",
            "Epoch 714/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1648 - accuracy: 0.7639 - val_loss: 0.2457 - val_accuracy: 0.6282\n",
            "Epoch 715/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1582 - accuracy: 0.7783 - val_loss: 0.2431 - val_accuracy: 0.6173\n",
            "Epoch 716/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1618 - accuracy: 0.7783 - val_loss: 0.2548 - val_accuracy: 0.6029\n",
            "Epoch 717/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1526 - accuracy: 0.7952 - val_loss: 0.2491 - val_accuracy: 0.6426\n",
            "Epoch 718/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1560 - accuracy: 0.7855 - val_loss: 0.2507 - val_accuracy: 0.6318\n",
            "Epoch 719/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1560 - accuracy: 0.7843 - val_loss: 0.2548 - val_accuracy: 0.6065\n",
            "Epoch 720/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1578 - accuracy: 0.7795 - val_loss: 0.2461 - val_accuracy: 0.6318\n",
            "Epoch 721/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1542 - accuracy: 0.7843 - val_loss: 0.2490 - val_accuracy: 0.6426\n",
            "Epoch 722/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1562 - accuracy: 0.7831 - val_loss: 0.2482 - val_accuracy: 0.6101\n",
            "Epoch 723/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1519 - accuracy: 0.7892 - val_loss: 0.2435 - val_accuracy: 0.6390\n",
            "Epoch 724/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1653 - accuracy: 0.7639 - val_loss: 0.2503 - val_accuracy: 0.6029\n",
            "Epoch 725/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1550 - accuracy: 0.7855 - val_loss: 0.2533 - val_accuracy: 0.6065\n",
            "Epoch 726/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1532 - accuracy: 0.7855 - val_loss: 0.2545 - val_accuracy: 0.6101\n",
            "Epoch 727/900\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1636 - accuracy: 0.7699 - val_loss: 0.2477 - val_accuracy: 0.6173\n",
            "Epoch 728/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1554 - accuracy: 0.7892 - val_loss: 0.2496 - val_accuracy: 0.6209\n",
            "Epoch 729/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1581 - accuracy: 0.7759 - val_loss: 0.2451 - val_accuracy: 0.6137\n",
            "Epoch 730/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1536 - accuracy: 0.7867 - val_loss: 0.2487 - val_accuracy: 0.6173\n",
            "Epoch 731/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1520 - accuracy: 0.7928 - val_loss: 0.2458 - val_accuracy: 0.6137\n",
            "Epoch 732/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1584 - accuracy: 0.7735 - val_loss: 0.2493 - val_accuracy: 0.6065\n",
            "Epoch 733/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1560 - accuracy: 0.7904 - val_loss: 0.2592 - val_accuracy: 0.6029\n",
            "Epoch 734/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1573 - accuracy: 0.7892 - val_loss: 0.2502 - val_accuracy: 0.5957\n",
            "Epoch 735/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1594 - accuracy: 0.7771 - val_loss: 0.2543 - val_accuracy: 0.6209\n",
            "Epoch 736/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1543 - accuracy: 0.7807 - val_loss: 0.2454 - val_accuracy: 0.6101\n",
            "Epoch 737/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1734 - accuracy: 0.7386 - val_loss: 0.2576 - val_accuracy: 0.6065\n",
            "Epoch 738/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1612 - accuracy: 0.7735 - val_loss: 0.2459 - val_accuracy: 0.6245\n",
            "Epoch 739/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1520 - accuracy: 0.7904 - val_loss: 0.2548 - val_accuracy: 0.6029\n",
            "Epoch 740/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1519 - accuracy: 0.7892 - val_loss: 0.2512 - val_accuracy: 0.6101\n",
            "Epoch 741/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1533 - accuracy: 0.7880 - val_loss: 0.2550 - val_accuracy: 0.6101\n",
            "Epoch 742/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1562 - accuracy: 0.7819 - val_loss: 0.2474 - val_accuracy: 0.6318\n",
            "Epoch 743/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1539 - accuracy: 0.7831 - val_loss: 0.2513 - val_accuracy: 0.6173\n",
            "Epoch 744/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1518 - accuracy: 0.7904 - val_loss: 0.2527 - val_accuracy: 0.6209\n",
            "Epoch 745/900\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1512 - accuracy: 0.7952 - val_loss: 0.2503 - val_accuracy: 0.6209\n",
            "Epoch 746/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1513 - accuracy: 0.7952 - val_loss: 0.2561 - val_accuracy: 0.6137\n",
            "Epoch 747/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1559 - accuracy: 0.7795 - val_loss: 0.2458 - val_accuracy: 0.6282\n",
            "Epoch 748/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1527 - accuracy: 0.7904 - val_loss: 0.2524 - val_accuracy: 0.6137\n",
            "Epoch 749/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1545 - accuracy: 0.7892 - val_loss: 0.2526 - val_accuracy: 0.6101\n",
            "Epoch 750/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1526 - accuracy: 0.7880 - val_loss: 0.2508 - val_accuracy: 0.6245\n",
            "Epoch 751/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1606 - accuracy: 0.7711 - val_loss: 0.2514 - val_accuracy: 0.6173\n",
            "Epoch 752/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1524 - accuracy: 0.7819 - val_loss: 0.2525 - val_accuracy: 0.6137\n",
            "Epoch 753/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1540 - accuracy: 0.7867 - val_loss: 0.2461 - val_accuracy: 0.6245\n",
            "Epoch 754/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1519 - accuracy: 0.7892 - val_loss: 0.2498 - val_accuracy: 0.6173\n",
            "Epoch 755/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1529 - accuracy: 0.7928 - val_loss: 0.2515 - val_accuracy: 0.5993\n",
            "Epoch 756/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1542 - accuracy: 0.7976 - val_loss: 0.2566 - val_accuracy: 0.6173\n",
            "Epoch 757/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1615 - accuracy: 0.7831 - val_loss: 0.2483 - val_accuracy: 0.6137\n",
            "Epoch 758/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1530 - accuracy: 0.7892 - val_loss: 0.2524 - val_accuracy: 0.6209\n",
            "Epoch 759/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1529 - accuracy: 0.7904 - val_loss: 0.2464 - val_accuracy: 0.6282\n",
            "Epoch 760/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1549 - accuracy: 0.7843 - val_loss: 0.2510 - val_accuracy: 0.6173\n",
            "Epoch 761/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1687 - accuracy: 0.7566 - val_loss: 0.2546 - val_accuracy: 0.6029\n",
            "Epoch 762/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1601 - accuracy: 0.7747 - val_loss: 0.2547 - val_accuracy: 0.6065\n",
            "Epoch 763/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1520 - accuracy: 0.7928 - val_loss: 0.2495 - val_accuracy: 0.6101\n",
            "Epoch 764/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1609 - accuracy: 0.7723 - val_loss: 0.2469 - val_accuracy: 0.6426\n",
            "Epoch 765/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1657 - accuracy: 0.7602 - val_loss: 0.2561 - val_accuracy: 0.5812\n",
            "Epoch 766/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1569 - accuracy: 0.7759 - val_loss: 0.2539 - val_accuracy: 0.6173\n",
            "Epoch 767/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1529 - accuracy: 0.7855 - val_loss: 0.2525 - val_accuracy: 0.6065\n",
            "Epoch 768/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1559 - accuracy: 0.7880 - val_loss: 0.2563 - val_accuracy: 0.6137\n",
            "Epoch 769/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1565 - accuracy: 0.7819 - val_loss: 0.2530 - val_accuracy: 0.6245\n",
            "Epoch 770/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.7783 - val_loss: 0.2509 - val_accuracy: 0.6065\n",
            "Epoch 771/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1549 - accuracy: 0.7831 - val_loss: 0.2508 - val_accuracy: 0.6173\n",
            "Epoch 772/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1533 - accuracy: 0.7855 - val_loss: 0.2484 - val_accuracy: 0.6101\n",
            "Epoch 773/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1548 - accuracy: 0.7855 - val_loss: 0.2515 - val_accuracy: 0.6065\n",
            "Epoch 774/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1526 - accuracy: 0.7867 - val_loss: 0.2490 - val_accuracy: 0.6354\n",
            "Epoch 775/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1616 - accuracy: 0.7675 - val_loss: 0.2538 - val_accuracy: 0.6209\n",
            "Epoch 776/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1533 - accuracy: 0.7807 - val_loss: 0.2539 - val_accuracy: 0.6173\n",
            "Epoch 777/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1584 - accuracy: 0.7843 - val_loss: 0.2502 - val_accuracy: 0.5957\n",
            "Epoch 778/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1541 - accuracy: 0.7904 - val_loss: 0.2551 - val_accuracy: 0.6173\n",
            "Epoch 779/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1535 - accuracy: 0.7795 - val_loss: 0.2535 - val_accuracy: 0.6173\n",
            "Epoch 780/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1516 - accuracy: 0.7916 - val_loss: 0.2548 - val_accuracy: 0.6137\n",
            "Epoch 781/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1507 - accuracy: 0.7976 - val_loss: 0.2530 - val_accuracy: 0.6173\n",
            "Epoch 782/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1501 - accuracy: 0.7952 - val_loss: 0.2544 - val_accuracy: 0.6209\n",
            "Epoch 783/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1505 - accuracy: 0.7940 - val_loss: 0.2544 - val_accuracy: 0.6209\n",
            "Epoch 784/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1492 - accuracy: 0.8000 - val_loss: 0.2522 - val_accuracy: 0.6101\n",
            "Epoch 785/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1505 - accuracy: 0.7892 - val_loss: 0.2529 - val_accuracy: 0.6173\n",
            "Epoch 786/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1550 - accuracy: 0.7855 - val_loss: 0.2552 - val_accuracy: 0.6137\n",
            "Epoch 787/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1555 - accuracy: 0.7831 - val_loss: 0.2474 - val_accuracy: 0.6209\n",
            "Epoch 788/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1794 - accuracy: 0.7313 - val_loss: 0.2535 - val_accuracy: 0.6209\n",
            "Epoch 789/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1852 - accuracy: 0.7446 - val_loss: 0.2578 - val_accuracy: 0.5776\n",
            "Epoch 790/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1648 - accuracy: 0.7639 - val_loss: 0.2487 - val_accuracy: 0.6173\n",
            "Epoch 791/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1523 - accuracy: 0.7916 - val_loss: 0.2464 - val_accuracy: 0.6245\n",
            "Epoch 792/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1534 - accuracy: 0.7892 - val_loss: 0.2508 - val_accuracy: 0.6029\n",
            "Epoch 793/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1526 - accuracy: 0.7831 - val_loss: 0.2488 - val_accuracy: 0.6282\n",
            "Epoch 794/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1555 - accuracy: 0.7795 - val_loss: 0.2467 - val_accuracy: 0.6282\n",
            "Epoch 795/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1567 - accuracy: 0.7735 - val_loss: 0.2495 - val_accuracy: 0.6173\n",
            "Epoch 796/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1533 - accuracy: 0.7807 - val_loss: 0.2497 - val_accuracy: 0.6173\n",
            "Epoch 797/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1518 - accuracy: 0.7952 - val_loss: 0.2480 - val_accuracy: 0.6137\n",
            "Epoch 798/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1528 - accuracy: 0.7843 - val_loss: 0.2507 - val_accuracy: 0.6065\n",
            "Epoch 799/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1508 - accuracy: 0.7831 - val_loss: 0.2510 - val_accuracy: 0.6245\n",
            "Epoch 800/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1604 - accuracy: 0.7759 - val_loss: 0.2584 - val_accuracy: 0.5884\n",
            "Epoch 801/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1565 - accuracy: 0.7807 - val_loss: 0.2549 - val_accuracy: 0.6282\n",
            "Epoch 802/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1519 - accuracy: 0.7928 - val_loss: 0.2507 - val_accuracy: 0.6065\n",
            "Epoch 803/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1560 - accuracy: 0.7916 - val_loss: 0.2476 - val_accuracy: 0.6426\n",
            "Epoch 804/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1527 - accuracy: 0.7904 - val_loss: 0.2501 - val_accuracy: 0.6029\n",
            "Epoch 805/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1518 - accuracy: 0.7892 - val_loss: 0.2535 - val_accuracy: 0.6209\n",
            "Epoch 806/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1503 - accuracy: 0.7964 - val_loss: 0.2473 - val_accuracy: 0.6173\n",
            "Epoch 807/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1530 - accuracy: 0.7928 - val_loss: 0.2500 - val_accuracy: 0.6390\n",
            "Epoch 808/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1543 - accuracy: 0.7819 - val_loss: 0.2486 - val_accuracy: 0.6390\n",
            "Epoch 809/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1525 - accuracy: 0.7952 - val_loss: 0.2482 - val_accuracy: 0.6173\n",
            "Epoch 810/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1548 - accuracy: 0.7831 - val_loss: 0.2554 - val_accuracy: 0.6209\n",
            "Epoch 811/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1518 - accuracy: 0.7867 - val_loss: 0.2600 - val_accuracy: 0.6101\n",
            "Epoch 812/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1561 - accuracy: 0.7699 - val_loss: 0.2556 - val_accuracy: 0.6245\n",
            "Epoch 813/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1528 - accuracy: 0.7867 - val_loss: 0.2546 - val_accuracy: 0.5993\n",
            "Epoch 814/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1537 - accuracy: 0.7880 - val_loss: 0.2523 - val_accuracy: 0.5993\n",
            "Epoch 815/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1498 - accuracy: 0.7940 - val_loss: 0.2520 - val_accuracy: 0.6137\n",
            "Epoch 816/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1500 - accuracy: 0.7964 - val_loss: 0.2506 - val_accuracy: 0.6137\n",
            "Epoch 817/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1515 - accuracy: 0.7867 - val_loss: 0.2504 - val_accuracy: 0.6354\n",
            "Epoch 818/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1543 - accuracy: 0.7783 - val_loss: 0.2548 - val_accuracy: 0.6209\n",
            "Epoch 819/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1513 - accuracy: 0.7976 - val_loss: 0.2544 - val_accuracy: 0.5957\n",
            "Epoch 820/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1567 - accuracy: 0.7747 - val_loss: 0.2536 - val_accuracy: 0.6209\n",
            "Epoch 821/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1554 - accuracy: 0.7795 - val_loss: 0.2557 - val_accuracy: 0.5848\n",
            "Epoch 822/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1590 - accuracy: 0.7759 - val_loss: 0.2547 - val_accuracy: 0.6245\n",
            "Epoch 823/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1614 - accuracy: 0.7687 - val_loss: 0.2501 - val_accuracy: 0.6245\n",
            "Epoch 824/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1611 - accuracy: 0.7711 - val_loss: 0.2491 - val_accuracy: 0.6173\n",
            "Epoch 825/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1545 - accuracy: 0.7880 - val_loss: 0.2464 - val_accuracy: 0.6029\n",
            "Epoch 826/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1501 - accuracy: 0.7880 - val_loss: 0.2545 - val_accuracy: 0.6065\n",
            "Epoch 827/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1501 - accuracy: 0.7940 - val_loss: 0.2484 - val_accuracy: 0.6282\n",
            "Epoch 828/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.7952 - val_loss: 0.2501 - val_accuracy: 0.6029\n",
            "Epoch 829/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1487 - accuracy: 0.7964 - val_loss: 0.2513 - val_accuracy: 0.6245\n",
            "Epoch 830/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.7759 - val_loss: 0.2512 - val_accuracy: 0.6209\n",
            "Epoch 831/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1513 - accuracy: 0.7952 - val_loss: 0.2490 - val_accuracy: 0.6245\n",
            "Epoch 832/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1509 - accuracy: 0.7916 - val_loss: 0.2528 - val_accuracy: 0.6318\n",
            "Epoch 833/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1561 - accuracy: 0.7747 - val_loss: 0.2461 - val_accuracy: 0.6318\n",
            "Epoch 834/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1540 - accuracy: 0.7843 - val_loss: 0.2493 - val_accuracy: 0.6029\n",
            "Epoch 835/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1534 - accuracy: 0.7880 - val_loss: 0.2509 - val_accuracy: 0.6245\n",
            "Epoch 836/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1510 - accuracy: 0.7952 - val_loss: 0.2470 - val_accuracy: 0.6065\n",
            "Epoch 837/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1518 - accuracy: 0.7867 - val_loss: 0.2537 - val_accuracy: 0.6101\n",
            "Epoch 838/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1492 - accuracy: 0.7904 - val_loss: 0.2472 - val_accuracy: 0.6318\n",
            "Epoch 839/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1638 - accuracy: 0.7675 - val_loss: 0.2593 - val_accuracy: 0.6029\n",
            "Epoch 840/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1561 - accuracy: 0.7807 - val_loss: 0.2513 - val_accuracy: 0.6173\n",
            "Epoch 841/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1500 - accuracy: 0.7940 - val_loss: 0.2462 - val_accuracy: 0.6101\n",
            "Epoch 842/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1487 - accuracy: 0.7928 - val_loss: 0.2501 - val_accuracy: 0.6065\n",
            "Epoch 843/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1494 - accuracy: 0.7831 - val_loss: 0.2510 - val_accuracy: 0.6173\n",
            "Epoch 844/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.7952 - val_loss: 0.2521 - val_accuracy: 0.6173\n",
            "Epoch 845/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1475 - accuracy: 0.8000 - val_loss: 0.2518 - val_accuracy: 0.6173\n",
            "Epoch 846/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1476 - accuracy: 0.8000 - val_loss: 0.2515 - val_accuracy: 0.6065\n",
            "Epoch 847/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1547 - accuracy: 0.7807 - val_loss: 0.2559 - val_accuracy: 0.6173\n",
            "Epoch 848/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.7711 - val_loss: 0.2525 - val_accuracy: 0.5993\n",
            "Epoch 849/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1522 - accuracy: 0.7831 - val_loss: 0.2530 - val_accuracy: 0.6173\n",
            "Epoch 850/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1485 - accuracy: 0.7916 - val_loss: 0.2511 - val_accuracy: 0.5993\n",
            "Epoch 851/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.7831 - val_loss: 0.2491 - val_accuracy: 0.6137\n",
            "Epoch 852/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.7928 - val_loss: 0.2496 - val_accuracy: 0.6390\n",
            "Epoch 853/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1523 - accuracy: 0.7964 - val_loss: 0.2482 - val_accuracy: 0.6245\n",
            "Epoch 854/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1569 - accuracy: 0.7795 - val_loss: 0.2585 - val_accuracy: 0.5993\n",
            "Epoch 855/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1508 - accuracy: 0.7952 - val_loss: 0.2486 - val_accuracy: 0.6137\n",
            "Epoch 856/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1539 - accuracy: 0.7892 - val_loss: 0.2511 - val_accuracy: 0.6245\n",
            "Epoch 857/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1523 - accuracy: 0.7843 - val_loss: 0.2448 - val_accuracy: 0.6065\n",
            "Epoch 858/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1510 - accuracy: 0.7831 - val_loss: 0.2528 - val_accuracy: 0.5957\n",
            "Epoch 859/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1628 - accuracy: 0.7627 - val_loss: 0.2487 - val_accuracy: 0.6282\n",
            "Epoch 860/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1581 - accuracy: 0.7771 - val_loss: 0.2477 - val_accuracy: 0.6173\n",
            "Epoch 861/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1514 - accuracy: 0.7916 - val_loss: 0.2483 - val_accuracy: 0.6318\n",
            "Epoch 862/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1511 - accuracy: 0.7831 - val_loss: 0.2455 - val_accuracy: 0.6173\n",
            "Epoch 863/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1484 - accuracy: 0.7928 - val_loss: 0.2474 - val_accuracy: 0.6101\n",
            "Epoch 864/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1498 - accuracy: 0.7916 - val_loss: 0.2494 - val_accuracy: 0.6101\n",
            "Epoch 865/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1506 - accuracy: 0.7928 - val_loss: 0.2515 - val_accuracy: 0.6209\n",
            "Epoch 866/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1497 - accuracy: 0.7964 - val_loss: 0.2480 - val_accuracy: 0.6282\n",
            "Epoch 867/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1473 - accuracy: 0.7976 - val_loss: 0.2498 - val_accuracy: 0.6065\n",
            "Epoch 868/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1485 - accuracy: 0.7940 - val_loss: 0.2557 - val_accuracy: 0.6137\n",
            "Epoch 869/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1485 - accuracy: 0.7940 - val_loss: 0.2493 - val_accuracy: 0.6065\n",
            "Epoch 870/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.7904 - val_loss: 0.2492 - val_accuracy: 0.6390\n",
            "Epoch 871/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1527 - accuracy: 0.7831 - val_loss: 0.2486 - val_accuracy: 0.6209\n",
            "Epoch 872/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1639 - accuracy: 0.7687 - val_loss: 0.2522 - val_accuracy: 0.6245\n",
            "Epoch 873/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1708 - accuracy: 0.7578 - val_loss: 0.2534 - val_accuracy: 0.6029\n",
            "Epoch 874/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1587 - accuracy: 0.7783 - val_loss: 0.2470 - val_accuracy: 0.6318\n",
            "Epoch 875/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1555 - accuracy: 0.7771 - val_loss: 0.2464 - val_accuracy: 0.6354\n",
            "Epoch 876/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1526 - accuracy: 0.7892 - val_loss: 0.2588 - val_accuracy: 0.6065\n",
            "Epoch 877/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1567 - accuracy: 0.7759 - val_loss: 0.2496 - val_accuracy: 0.5993\n",
            "Epoch 878/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1529 - accuracy: 0.7795 - val_loss: 0.2490 - val_accuracy: 0.6173\n",
            "Epoch 879/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1501 - accuracy: 0.7940 - val_loss: 0.2502 - val_accuracy: 0.6318\n",
            "Epoch 880/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1855 - accuracy: 0.7410 - val_loss: 0.2508 - val_accuracy: 0.6029\n",
            "Epoch 881/900\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1648 - accuracy: 0.7687 - val_loss: 0.2519 - val_accuracy: 0.6245\n",
            "Epoch 882/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1677 - accuracy: 0.7639 - val_loss: 0.2508 - val_accuracy: 0.6354\n",
            "Epoch 883/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1653 - accuracy: 0.7590 - val_loss: 0.2568 - val_accuracy: 0.5993\n",
            "Epoch 884/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1672 - accuracy: 0.7518 - val_loss: 0.2455 - val_accuracy: 0.6390\n",
            "Epoch 885/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1622 - accuracy: 0.7723 - val_loss: 0.2426 - val_accuracy: 0.6245\n",
            "Epoch 886/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1552 - accuracy: 0.7819 - val_loss: 0.2493 - val_accuracy: 0.6101\n",
            "Epoch 887/900\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1498 - accuracy: 0.7940 - val_loss: 0.2447 - val_accuracy: 0.6426\n",
            "Epoch 888/900\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.7916 - val_loss: 0.2513 - val_accuracy: 0.6065\n",
            "Epoch 889/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1481 - accuracy: 0.7952 - val_loss: 0.2479 - val_accuracy: 0.6209\n",
            "Epoch 890/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1527 - accuracy: 0.7892 - val_loss: 0.2460 - val_accuracy: 0.6065\n",
            "Epoch 891/900\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1530 - accuracy: 0.7831 - val_loss: 0.2455 - val_accuracy: 0.6245\n",
            "Epoch 892/900\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.1552 - accuracy: 0.7723 - val_loss: 0.2506 - val_accuracy: 0.6065\n",
            "Epoch 893/900\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1508 - accuracy: 0.7843 - val_loss: 0.2469 - val_accuracy: 0.6282\n",
            "Epoch 894/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1490 - accuracy: 0.7867 - val_loss: 0.2514 - val_accuracy: 0.6173\n",
            "Epoch 895/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.7916 - val_loss: 0.2571 - val_accuracy: 0.6029\n",
            "Epoch 896/900\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1524 - accuracy: 0.7819 - val_loss: 0.2508 - val_accuracy: 0.6029\n",
            "Epoch 897/900\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1601 - accuracy: 0.7639 - val_loss: 0.2538 - val_accuracy: 0.6209\n",
            "Epoch 898/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1518 - accuracy: 0.7904 - val_loss: 0.2526 - val_accuracy: 0.5848\n",
            "Epoch 899/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1580 - accuracy: 0.7843 - val_loss: 0.2484 - val_accuracy: 0.6390\n",
            "Epoch 900/900\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.1509 - accuracy: 0.7867 - val_loss: 0.2476 - val_accuracy: 0.6173\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train, batch_size=55, epochs=900, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAljRCqZZ3eD",
        "outputId": "fceeb6ac-68d5-4107-fc8b-f4c564848d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred=y_pred>0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeazVcmBap13",
        "outputId": "694f333f-b8ad-408b-aa9f-3e852ef40cdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 71,  48],\n",
              "       [ 58, 100]])"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "wQOb60PobC3C",
        "outputId": "4535159d-2d89-4691-8bf6-a2b1f03fc536"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 31.222222222222214, 'Predicted label')"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x550 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3GsMcNkblRU",
        "outputId": "9974852a-78fe-4010-8d1e-a9a32bd39878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6173285198555957\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "\n",
        "acc = (cm[0][0] + cm[1][1]) / numpy.sum(cm)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnKmDfxya3EZ"
      },
      "source": [
        "<H1> Final Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "XIy9EQySZhD4",
        "outputId": "d9755c2c-b9a8-4410-ccaa-02950e9394a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_5928e_row0_col1 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_5928e_row1_col1 {\n",
              "  background-color: #e4e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_5928e_row2_col1 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_5928e_row3_col1 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_5928e_row4_col1 {\n",
              "  background-color: #67a4cc;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_5928e_row5_col1 {\n",
              "  background-color: #60a1ca;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_5928e_row6_col1 {\n",
              "  background-color: #167bb6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_5928e_row7_col1 {\n",
              "  background-color: #04649d;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_5928e_row8_col1 {\n",
              "  background-color: #023858;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_5928e\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_5928e_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_5928e_level0_col1\" class=\"col_heading level0 col1\" >Accuracy_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row0\" class=\"row_heading level0 row0\" >6</th>\n",
              "      <td id=\"T_5928e_row0_col0\" class=\"data row0 col0\" >Logistic Regression</td>\n",
              "      <td id=\"T_5928e_row0_col1\" class=\"data row0 col1\" >0.581227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row1\" class=\"row_heading level0 row1\" >8</th>\n",
              "      <td id=\"T_5928e_row1_col0\" class=\"data row1 col0\" >ANN</td>\n",
              "      <td id=\"T_5928e_row1_col1\" class=\"data row1 col1\" >0.617329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row2\" class=\"row_heading level0 row2\" >7</th>\n",
              "      <td id=\"T_5928e_row2_col0\" class=\"data row2 col0\" >AdaBoostClassifier</td>\n",
              "      <td id=\"T_5928e_row2_col1\" class=\"data row2 col1\" >0.628159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row3\" class=\"row_heading level0 row3\" >2</th>\n",
              "      <td id=\"T_5928e_row3_col0\" class=\"data row3 col0\" >DecisionTreeClassifier</td>\n",
              "      <td id=\"T_5928e_row3_col1\" class=\"data row3 col1\" >0.675090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_5928e_row4_col0\" class=\"data row4 col0\" >KNN</td>\n",
              "      <td id=\"T_5928e_row4_col1\" class=\"data row4 col1\" >0.696751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_5928e_row5_col0\" class=\"data row5 col0\" >SVM</td>\n",
              "      <td id=\"T_5928e_row5_col1\" class=\"data row5 col1\" >0.700361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row6\" class=\"row_heading level0 row6\" >3</th>\n",
              "      <td id=\"T_5928e_row6_col0\" class=\"data row6 col0\" >RandomForestClassifier</td>\n",
              "      <td id=\"T_5928e_row6_col1\" class=\"data row6 col1\" >0.736462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row7\" class=\"row_heading level0 row7\" >1</th>\n",
              "      <td id=\"T_5928e_row7_col0\" class=\"data row7 col0\" >BaggingClassifier</td>\n",
              "      <td id=\"T_5928e_row7_col1\" class=\"data row7 col1\" >0.761733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_5928e_level0_row8\" class=\"row_heading level0 row8\" >0</th>\n",
              "      <td id=\"T_5928e_row8_col0\" class=\"data row8 col0\" >GradientBoostingClassifier</td>\n",
              "      <td id=\"T_5928e_row8_col1\" class=\"data row8 col1\" >0.801444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7e79ce5e73a0>"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models = pd.DataFrame({\n",
        "    'Model':['GradientBoostingClassifier','BaggingClassifier','DecisionTreeClassifier','RandomForestClassifier','KNN','SVM','Logistic Regression','AdaBoostClassifier','ANN'],\n",
        "    'Accuracy_score' :[gb, bc, dt, rf, knn, svm, lr,ab,acc]\n",
        "})\n",
        "models.sort_values(by=['Accuracy_score'], ascending=True).style.background_gradient(subset=['Accuracy_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmppfukjipe6"
      },
      "source": [
        "## **Our accuracy varies from 78% to 82% due to the following reasons:**::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlfnfxLfiPnX"
      },
      "source": [
        "**Randomness in SMOTE and Data Splitting**: SMOTE involves selecting random minority class samples and creating synthetic examples based on their nearest neighbors. Depending on the random sampling of these instances and the choice of nearest neighbors, you can get slightly different synthetic data points each time you run the code.\n",
        "\n",
        "**Data Split Variability:** If you are splitting your data into training and testing subsets, there is randomness in the data split process as well. The choice of which instances go into the training set and which go into the test set can vary from one run to another.\n",
        "\n",
        "**Model Initialization:** Many machine learning models have random initialization of weights. The initial conditions of the model can influence how it converges during training. Consequently, small differences in initial conditions can lead to variations in the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEH8blNRjB4I"
      },
      "source": [
        "## **To mitigate the variability in accuracy when using SMOTE-ENN or other techniques, consider the following:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Ce7TaMi8xi"
      },
      "source": [
        "- Perform multiple runs and calculate the average and standard deviation of accuracy scores to get a more robust estimate of the model's performance.\n",
        "- Use techniques like k-fold cross-validation, which can help reduce the impact of random variability in the data split.\n",
        "- Ensure that you set random seeds or random_state parameters consistently to make your results reproducible when debugging and comparing different runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsU1k6qgbbDy"
      },
      "source": [
        "<h1> H20 Auto ML - StackedEnsemble model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO0FUx7Gbzyv",
        "outputId": "c1c7d5cb-a93e-4772-b76e-2289c3c0de8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html\n",
            "Requirement already satisfied: h2o in /usr/local/lib/python3.10/dist-packages (3.44.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.31.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "dLuraQo6bvzh",
        "outputId": "48d08022-c532-48a7-bdd0-16254a10a3e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.20.1\" 2023-08-24; OpenJDK Runtime Environment (build 11.0.20.1+1-post-Ubuntu-0ubuntu122.04); OpenJDK 64-Bit Server VM (build 11.0.20.1+1-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpwf7o9lpr\n",
            "  JVM stdout: /tmp/tmpwf7o9lpr/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpwf7o9lpr/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-1.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-1 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-1 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-1 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table th,\n",
              "#h2o-table-1 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>05 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.44.0.1</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>8 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_duqyr3</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>2 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ],
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         05 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.44.0.1\n",
              "H2O_cluster_version_age:    8 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_duqyr3\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    2 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "h2o.init(max_mem_size='2G')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uc-PnJhrgZ6",
        "outputId": "bae23bc2-c8f0-4e06-ac6f-09a265f91cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/water_potability.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwlgAxbxcDc-",
        "outputId": "949b6a74-664f-490a-8138-7698d723becd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ],
      "source": [
        "h2o_df = h2o.H2OFrame(df)\n",
        "h2o_df['Potability']=h2o_df['Potability'].asfactor()\n",
        "X=h2o_df.columns[0:-1]\n",
        "y=h2o_df.columns[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPmH4rkDcK3I",
        "outputId": "ba2a0aa6-72b7-4598-eeba-0e50cf3dce9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2285\n",
            "991\n"
          ]
        }
      ],
      "source": [
        "train, test=h2o_df.split_frame(ratios=[.7])\n",
        "print(train.nrows)\n",
        "print(test.nrows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM7mcBY7cPPE",
        "outputId": "424fa5d5-8d9d-4ff7-dfb2-8ee681899462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoML progress: |███████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
              "=============\n",
              "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
              "Model Key: StackedEnsemble_BestOfFamily_4_AutoML_2_20231025_41020\n",
              "</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-2.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-2 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-2 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-2 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table th,\n",
              "#h2o-table-2 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Model Summary for Stacked Ensemble: </caption>\n",
              "    <thead><tr><th>key</th>\n",
              "<th>value</th></tr></thead>\n",
              "    <tbody><tr><td>Stacking strategy</td>\n",
              "<td>cross_validation</td></tr>\n",
              "<tr><td>Number of base models (used / total)</td>\n",
              "<td>4/6</td></tr>\n",
              "<tr><td># GBM base models (used / total)</td>\n",
              "<td>1/1</td></tr>\n",
              "<tr><td># XGBoost base models (used / total)</td>\n",
              "<td>0/1</td></tr>\n",
              "<tr><td># DeepLearning base models (used / total)</td>\n",
              "<td>1/1</td></tr>\n",
              "<tr><td># DRF base models (used / total)</td>\n",
              "<td>2/2</td></tr>\n",
              "<tr><td># GLM base models (used / total)</td>\n",
              "<td>0/1</td></tr>\n",
              "<tr><td>Metalearner algorithm</td>\n",
              "<td>GLM</td></tr>\n",
              "<tr><td>Metalearner fold assignment scheme</td>\n",
              "<td>Random</td></tr>\n",
              "<tr><td>Metalearner nfolds</td>\n",
              "<td>5</td></tr>\n",
              "<tr><td>Metalearner fold_column</td>\n",
              "<td>None</td></tr>\n",
              "<tr><td>Custom metalearner hyperparameters</td>\n",
              "<td>None</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.1031204083854706\n",
              "RMSE: 0.32112366525292185\n",
              "LogLoss: 0.3572143407453567\n",
              "AUC: 0.9768681046167664\n",
              "AUCPR: 0.9550685580539391\n",
              "Gini: 0.9537362092335329\n",
              "Null degrees of freedom: 2284\n",
              "Residual degrees of freedom: 2280\n",
              "Null deviance: 3048.7652424086486\n",
              "Residual deviance: 1632.46953720628\n",
              "AIC: 1642.46953720628</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-3.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-3 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-3 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-3 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-3 .h2o-table th,\n",
              "#h2o-table-3 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.39520420066275386</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>1301.0</td>\n",
              "<td>101.0</td>\n",
              "<td>0.072</td>\n",
              "<td> (101.0/1402.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>42.0</td>\n",
              "<td>841.0</td>\n",
              "<td>0.0476</td>\n",
              "<td> (42.0/883.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>1343.0</td>\n",
              "<td>942.0</td>\n",
              "<td>0.0626</td>\n",
              "<td> (143.0/2285.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-4.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-4 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-4 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-4 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-4 .h2o-table th,\n",
              "#h2o-table-4 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-4 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-4\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3952042</td>\n",
              "<td>0.9216438</td>\n",
              "<td>207.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.3571155</td>\n",
              "<td>0.9479718</td>\n",
              "<td>221.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.4664008</td>\n",
              "<td>0.9258809</td>\n",
              "<td>179.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.4284557</td>\n",
              "<td>0.9382932</td>\n",
              "<td>195.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9995186</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.1606338</td>\n",
              "<td>1.0</td>\n",
              "<td>333.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9995186</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.3952042</td>\n",
              "<td>0.8709003</td>\n",
              "<td>207.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.4150620</td>\n",
              "<td>0.9358060</td>\n",
              "<td>200.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3755651</td>\n",
              "<td>0.9410767</td>\n",
              "<td>213.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9995186</td>\n",
              "<td>1402.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9995186</td>\n",
              "<td>864.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0010875</td>\n",
              "<td>1402.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.1606338</td>\n",
              "<td>883.0</td>\n",
              "<td>333.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9995186</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9995186</td>\n",
              "<td>0.9784824</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0010875</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.1606338</td>\n",
              "<td>1.0</td>\n",
              "<td>333.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-5.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-5 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-5 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-5 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-5 .h2o-table th,\n",
              "#h2o-table-5 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 38.64 %, avg score: 39.46 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.0100656</td>\n",
              "<td>0.9968740</td>\n",
              "<td>2.5877690</td>\n",
              "<td>2.5877690</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9991779</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9991779</td>\n",
              "<td>0.0260476</td>\n",
              "<td>0.0260476</td>\n",
              "<td>158.7768969</td>\n",
              "<td>158.7768969</td>\n",
              "<td>0.0260476</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.0201313</td>\n",
              "<td>0.9823290</td>\n",
              "<td>2.4752573</td>\n",
              "<td>2.5315131</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.9907533</td>\n",
              "<td>0.9782609</td>\n",
              "<td>0.9949656</td>\n",
              "<td>0.0249151</td>\n",
              "<td>0.0509626</td>\n",
              "<td>147.5257275</td>\n",
              "<td>153.1513122</td>\n",
              "<td>0.0502494</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.0301969</td>\n",
              "<td>0.9578965</td>\n",
              "<td>2.5877690</td>\n",
              "<td>2.5502651</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9708087</td>\n",
              "<td>0.9855072</td>\n",
              "<td>0.9869133</td>\n",
              "<td>0.0260476</td>\n",
              "<td>0.0770102</td>\n",
              "<td>158.7768969</td>\n",
              "<td>155.0265071</td>\n",
              "<td>0.0762969</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.0402626</td>\n",
              "<td>0.9369026</td>\n",
              "<td>2.5877690</td>\n",
              "<td>2.5596410</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9479450</td>\n",
              "<td>0.9891304</td>\n",
              "<td>0.9771712</td>\n",
              "<td>0.0260476</td>\n",
              "<td>0.1030578</td>\n",
              "<td>158.7768969</td>\n",
              "<td>155.9641046</td>\n",
              "<td>0.1023445</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.0503282</td>\n",
              "<td>0.9021611</td>\n",
              "<td>2.5877690</td>\n",
              "<td>2.5652666</td>\n",
              "<td>1.0</td>\n",
              "<td>0.9196841</td>\n",
              "<td>0.9913043</td>\n",
              "<td>0.9656738</td>\n",
              "<td>0.0260476</td>\n",
              "<td>0.1291053</td>\n",
              "<td>158.7768969</td>\n",
              "<td>156.5266631</td>\n",
              "<td>0.1283921</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1002188</td>\n",
              "<td>0.7805109</td>\n",
              "<td>2.5196698</td>\n",
              "<td>2.5425678</td>\n",
              "<td>0.9736842</td>\n",
              "<td>0.8435097</td>\n",
              "<td>0.9825328</td>\n",
              "<td>0.9048585</td>\n",
              "<td>0.1257078</td>\n",
              "<td>0.2548131</td>\n",
              "<td>151.9669786</td>\n",
              "<td>154.2567765</td>\n",
              "<td>0.2519601</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.1501094</td>\n",
              "<td>0.6837115</td>\n",
              "<td>2.3607717</td>\n",
              "<td>2.4821457</td>\n",
              "<td>0.9122807</td>\n",
              "<td>0.7317854</td>\n",
              "<td>0.9591837</td>\n",
              "<td>0.8473356</td>\n",
              "<td>0.1177803</td>\n",
              "<td>0.3725934</td>\n",
              "<td>136.0771691</td>\n",
              "<td>148.2145746</td>\n",
              "<td>0.3626077</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.6119478</td>\n",
              "<td>2.5196698</td>\n",
              "<td>2.4915062</td>\n",
              "<td>0.9736842</td>\n",
              "<td>0.6470650</td>\n",
              "<td>0.9628009</td>\n",
              "<td>0.7973775</td>\n",
              "<td>0.1257078</td>\n",
              "<td>0.4983012</td>\n",
              "<td>151.9669786</td>\n",
              "<td>149.1506229</td>\n",
              "<td>0.4861757</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3002188</td>\n",
              "<td>0.5122309</td>\n",
              "<td>2.4408651</td>\n",
              "<td>2.4746012</td>\n",
              "<td>0.9432314</td>\n",
              "<td>0.5604087</td>\n",
              "<td>0.9562682</td>\n",
              "<td>0.7182728</td>\n",
              "<td>0.2446206</td>\n",
              "<td>0.7429219</td>\n",
              "<td>144.0865054</td>\n",
              "<td>147.4601230</td>\n",
              "<td>0.7215239</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.4152843</td>\n",
              "<td>1.9294769</td>\n",
              "<td>2.3386183</td>\n",
              "<td>0.7456140</td>\n",
              "<td>0.4686954</td>\n",
              "<td>0.9037199</td>\n",
              "<td>0.6560150</td>\n",
              "<td>0.1925255</td>\n",
              "<td>0.9354473</td>\n",
              "<td>92.9476863</td>\n",
              "<td>133.8618347</td>\n",
              "<td>0.8726799</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5002188</td>\n",
              "<td>0.3114101</td>\n",
              "<td>0.5198139</td>\n",
              "<td>1.9742209</td>\n",
              "<td>0.2008734</td>\n",
              "<td>0.3496885</td>\n",
              "<td>0.7629046</td>\n",
              "<td>0.5946425</td>\n",
              "<td>0.0520951</td>\n",
              "<td>0.9875425</td>\n",
              "<td>-48.0186146</td>\n",
              "<td>97.4220946</td>\n",
              "<td>0.7942472</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.2690698</td>\n",
              "<td>0.0794490</td>\n",
              "<td>1.6591166</td>\n",
              "<td>0.0307018</td>\n",
              "<td>0.2894162</td>\n",
              "<td>0.6411379</td>\n",
              "<td>0.5438827</td>\n",
              "<td>0.0079275</td>\n",
              "<td>0.9954700</td>\n",
              "<td>-92.0550953</td>\n",
              "<td>65.9116648</td>\n",
              "<td>0.6445427</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.6997812</td>\n",
              "<td>0.2305988</td>\n",
              "<td>0.0226997</td>\n",
              "<td>1.4257814</td>\n",
              "<td>0.0087719</td>\n",
              "<td>0.2492896</td>\n",
              "<td>0.5509694</td>\n",
              "<td>0.5018770</td>\n",
              "<td>0.0022650</td>\n",
              "<td>0.9977350</td>\n",
              "<td>-97.7300272</td>\n",
              "<td>42.5781402</td>\n",
              "<td>0.4856095</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.1878801</td>\n",
              "<td>0.0113003</td>\n",
              "<td>1.2485844</td>\n",
              "<td>0.0043668</td>\n",
              "<td>0.2097266</td>\n",
              "<td>0.4824945</td>\n",
              "<td>0.4652783</td>\n",
              "<td>0.0011325</td>\n",
              "<td>0.9988675</td>\n",
              "<td>-98.8699699</td>\n",
              "<td>24.8584371</td>\n",
              "<td>0.3241171</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.8997812</td>\n",
              "<td>0.1205931</td>\n",
              "<td>0.0113499</td>\n",
              "<td>1.1113813</td>\n",
              "<td>0.0043860</td>\n",
              "<td>0.1586448</td>\n",
              "<td>0.4294747</td>\n",
              "<td>0.4312742</td>\n",
              "<td>0.0011325</td>\n",
              "<td>1.0</td>\n",
              "<td>-98.8650136</td>\n",
              "<td>11.1381323</td>\n",
              "<td>0.1633381</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0003145</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0651503</td>\n",
              "<td>0.3864333</td>\n",
              "<td>0.3945817</td>\n",
              "<td>0.0</td>\n",
              "<td>1.0</td>\n",
              "<td>-100.0</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: stackedensemble\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.2062812126266294\n",
              "RMSE: 0.4541819157855467\n",
              "LogLoss: 0.5994868909822116\n",
              "AUC: 0.7002046098196557\n",
              "AUCPR: 0.6174190344601261\n",
              "Gini: 0.4004092196393114\n",
              "Null degrees of freedom: 2284\n",
              "Residual degrees of freedom: 2280\n",
              "Null deviance: 3049.4826908427112\n",
              "Residual deviance: 2739.655091788707\n",
              "AIC: 2749.655091788707</pre>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-6.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-6 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-6 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-6 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-6 .h2o-table th,\n",
              "#h2o-table-6 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.30834730182737674</caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>0</th>\n",
              "<th>1</th>\n",
              "<th>Error</th>\n",
              "<th>Rate</th></tr></thead>\n",
              "    <tbody><tr><td>0</td>\n",
              "<td>591.0</td>\n",
              "<td>811.0</td>\n",
              "<td>0.5785</td>\n",
              "<td> (811.0/1402.0)</td></tr>\n",
              "<tr><td>1</td>\n",
              "<td>154.0</td>\n",
              "<td>729.0</td>\n",
              "<td>0.1744</td>\n",
              "<td> (154.0/883.0)</td></tr>\n",
              "<tr><td>Total</td>\n",
              "<td>745.0</td>\n",
              "<td>1540.0</td>\n",
              "<td>0.4223</td>\n",
              "<td> (965.0/2285.0)</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-7.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-7 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-7 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-7 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-7 .h2o-table th,\n",
              "#h2o-table-7 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
              "    <thead><tr><th>metric</th>\n",
              "<th>threshold</th>\n",
              "<th>value</th>\n",
              "<th>idx</th></tr></thead>\n",
              "    <tbody><tr><td>max f1</td>\n",
              "<td>0.3083473</td>\n",
              "<td>0.6017334</td>\n",
              "<td>270.0</td></tr>\n",
              "<tr><td>max f2</td>\n",
              "<td>0.1689221</td>\n",
              "<td>0.7638271</td>\n",
              "<td>351.0</td></tr>\n",
              "<tr><td>max f0point5</td>\n",
              "<td>0.4707778</td>\n",
              "<td>0.5784989</td>\n",
              "<td>154.0</td></tr>\n",
              "<tr><td>max accuracy</td>\n",
              "<td>0.4707778</td>\n",
              "<td>0.6853392</td>\n",
              "<td>154.0</td></tr>\n",
              "<tr><td>max precision</td>\n",
              "<td>0.9993934</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max recall</td>\n",
              "<td>0.0586580</td>\n",
              "<td>1.0</td>\n",
              "<td>394.0</td></tr>\n",
              "<tr><td>max specificity</td>\n",
              "<td>0.9993934</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max absolute_mcc</td>\n",
              "<td>0.4707778</td>\n",
              "<td>0.3033167</td>\n",
              "<td>154.0</td></tr>\n",
              "<tr><td>max min_per_class_accuracy</td>\n",
              "<td>0.3716288</td>\n",
              "<td>0.6375991</td>\n",
              "<td>223.0</td></tr>\n",
              "<tr><td>max mean_per_class_accuracy</td>\n",
              "<td>0.3728493</td>\n",
              "<td>0.6405673</td>\n",
              "<td>222.0</td></tr>\n",
              "<tr><td>max tns</td>\n",
              "<td>0.9993934</td>\n",
              "<td>1402.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fns</td>\n",
              "<td>0.9993934</td>\n",
              "<td>882.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fps</td>\n",
              "<td>0.0336666</td>\n",
              "<td>1402.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tps</td>\n",
              "<td>0.0586580</td>\n",
              "<td>883.0</td>\n",
              "<td>394.0</td></tr>\n",
              "<tr><td>max tnr</td>\n",
              "<td>0.9993934</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fnr</td>\n",
              "<td>0.9993934</td>\n",
              "<td>0.9988675</td>\n",
              "<td>0.0</td></tr>\n",
              "<tr><td>max fpr</td>\n",
              "<td>0.0336666</td>\n",
              "<td>1.0</td>\n",
              "<td>399.0</td></tr>\n",
              "<tr><td>max tpr</td>\n",
              "<td>0.0586580</td>\n",
              "<td>1.0</td>\n",
              "<td>394.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-8.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-8 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-8 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-8 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-8 .h2o-table th,\n",
              "#h2o-table-8 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-8 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-8\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Gains/Lift Table: Avg response rate: 38.64 %, avg score: 38.63 %</caption>\n",
              "    <thead><tr><th>group</th>\n",
              "<th>cumulative_data_fraction</th>\n",
              "<th>lower_threshold</th>\n",
              "<th>lift</th>\n",
              "<th>cumulative_lift</th>\n",
              "<th>response_rate</th>\n",
              "<th>score</th>\n",
              "<th>cumulative_response_rate</th>\n",
              "<th>cumulative_score</th>\n",
              "<th>capture_rate</th>\n",
              "<th>cumulative_capture_rate</th>\n",
              "<th>gain</th>\n",
              "<th>cumulative_gain</th>\n",
              "<th>kolmogorov_smirnov</th></tr></thead>\n",
              "    <tbody><tr><td>1</td>\n",
              "<td>0.0100656</td>\n",
              "<td>0.8967957</td>\n",
              "<td>2.4752573</td>\n",
              "<td>2.4752573</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.9419048</td>\n",
              "<td>0.9565217</td>\n",
              "<td>0.9419048</td>\n",
              "<td>0.0249151</td>\n",
              "<td>0.0249151</td>\n",
              "<td>147.5257275</td>\n",
              "<td>147.5257275</td>\n",
              "<td>0.0242018</td></tr>\n",
              "<tr><td>2</td>\n",
              "<td>0.0201313</td>\n",
              "<td>0.8460488</td>\n",
              "<td>2.2502339</td>\n",
              "<td>2.3627456</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.8708598</td>\n",
              "<td>0.9130435</td>\n",
              "<td>0.9063823</td>\n",
              "<td>0.0226501</td>\n",
              "<td>0.0475651</td>\n",
              "<td>125.0233886</td>\n",
              "<td>136.2745581</td>\n",
              "<td>0.0447121</td></tr>\n",
              "<tr><td>3</td>\n",
              "<td>0.0301969</td>\n",
              "<td>0.8135411</td>\n",
              "<td>1.8001871</td>\n",
              "<td>2.1752261</td>\n",
              "<td>0.6956522</td>\n",
              "<td>0.8304912</td>\n",
              "<td>0.8405797</td>\n",
              "<td>0.8810853</td>\n",
              "<td>0.0181200</td>\n",
              "<td>0.0656852</td>\n",
              "<td>80.0187109</td>\n",
              "<td>117.5226090</td>\n",
              "<td>0.0578392</td></tr>\n",
              "<tr><td>4</td>\n",
              "<td>0.0402626</td>\n",
              "<td>0.7737520</td>\n",
              "<td>2.0252105</td>\n",
              "<td>2.1377222</td>\n",
              "<td>0.7826087</td>\n",
              "<td>0.7972523</td>\n",
              "<td>0.8260870</td>\n",
              "<td>0.8601270</td>\n",
              "<td>0.0203851</td>\n",
              "<td>0.0860702</td>\n",
              "<td>102.5210498</td>\n",
              "<td>113.7722192</td>\n",
              "<td>0.0746579</td></tr>\n",
              "<tr><td>5</td>\n",
              "<td>0.0503282</td>\n",
              "<td>0.7427705</td>\n",
              "<td>2.2502339</td>\n",
              "<td>2.1602245</td>\n",
              "<td>0.8695652</td>\n",
              "<td>0.7587003</td>\n",
              "<td>0.8347826</td>\n",
              "<td>0.8398417</td>\n",
              "<td>0.0226501</td>\n",
              "<td>0.1087203</td>\n",
              "<td>125.0233886</td>\n",
              "<td>116.0224531</td>\n",
              "<td>0.0951682</td></tr>\n",
              "<tr><td>6</td>\n",
              "<td>0.1002188</td>\n",
              "<td>0.6347724</td>\n",
              "<td>1.9294769</td>\n",
              "<td>2.0453545</td>\n",
              "<td>0.7456140</td>\n",
              "<td>0.6882756</td>\n",
              "<td>0.7903930</td>\n",
              "<td>0.7643896</td>\n",
              "<td>0.0962627</td>\n",
              "<td>0.2049830</td>\n",
              "<td>92.9476863</td>\n",
              "<td>104.5354513</td>\n",
              "<td>0.1707462</td></tr>\n",
              "<tr><td>7</td>\n",
              "<td>0.1501094</td>\n",
              "<td>0.5538389</td>\n",
              "<td>1.4073831</td>\n",
              "<td>1.8333174</td>\n",
              "<td>0.5438596</td>\n",
              "<td>0.5899006</td>\n",
              "<td>0.7084548</td>\n",
              "<td>0.7063962</td>\n",
              "<td>0.0702152</td>\n",
              "<td>0.2751982</td>\n",
              "<td>40.7383124</td>\n",
              "<td>83.3317375</td>\n",
              "<td>0.2038715</td></tr>\n",
              "<tr><td>8</td>\n",
              "<td>0.2</td>\n",
              "<td>0.4966708</td>\n",
              "<td>1.4073831</td>\n",
              "<td>1.7270668</td>\n",
              "<td>0.5438596</td>\n",
              "<td>0.5223176</td>\n",
              "<td>0.6673961</td>\n",
              "<td>0.6604772</td>\n",
              "<td>0.0702152</td>\n",
              "<td>0.3454134</td>\n",
              "<td>40.7383124</td>\n",
              "<td>72.7066818</td>\n",
              "<td>0.2369968</td></tr>\n",
              "<tr><td>9</td>\n",
              "<td>0.3002188</td>\n",
              "<td>0.4374427</td>\n",
              "<td>1.1300301</td>\n",
              "<td>1.5277645</td>\n",
              "<td>0.4366812</td>\n",
              "<td>0.4658684</td>\n",
              "<td>0.5903790</td>\n",
              "<td>0.5955130</td>\n",
              "<td>0.1132503</td>\n",
              "<td>0.4586636</td>\n",
              "<td>13.0030118</td>\n",
              "<td>52.7764479</td>\n",
              "<td>0.2582357</td></tr>\n",
              "<tr><td>10</td>\n",
              "<td>0.4</td>\n",
              "<td>0.3954767</td>\n",
              "<td>1.1236365</td>\n",
              "<td>1.4269536</td>\n",
              "<td>0.4342105</td>\n",
              "<td>0.4154170</td>\n",
              "<td>0.5514223</td>\n",
              "<td>0.5505875</td>\n",
              "<td>0.1121178</td>\n",
              "<td>0.5707814</td>\n",
              "<td>12.3636526</td>\n",
              "<td>42.6953567</td>\n",
              "<td>0.2783421</td></tr>\n",
              "<tr><td>11</td>\n",
              "<td>0.5002188</td>\n",
              "<td>0.3626592</td>\n",
              "<td>0.9492253</td>\n",
              "<td>1.3312407</td>\n",
              "<td>0.3668122</td>\n",
              "<td>0.3770351</td>\n",
              "<td>0.5144357</td>\n",
              "<td>0.5158163</td>\n",
              "<td>0.0951302</td>\n",
              "<td>0.6659117</td>\n",
              "<td>-5.0774701</td>\n",
              "<td>33.1240730</td>\n",
              "<td>0.2700486</td></tr>\n",
              "<tr><td>12</td>\n",
              "<td>0.6</td>\n",
              "<td>0.3313129</td>\n",
              "<td>0.8966392</td>\n",
              "<td>1.2589656</td>\n",
              "<td>0.3464912</td>\n",
              "<td>0.3473328</td>\n",
              "<td>0.4865062</td>\n",
              "<td>0.4877972</td>\n",
              "<td>0.0894677</td>\n",
              "<td>0.7553794</td>\n",
              "<td>-10.3360752</td>\n",
              "<td>25.8965647</td>\n",
              "<td>0.2532396</td></tr>\n",
              "<tr><td>13</td>\n",
              "<td>0.6997812</td>\n",
              "<td>0.2976337</td>\n",
              "<td>0.8512398</td>\n",
              "<td>1.2008284</td>\n",
              "<td>0.3289474</td>\n",
              "<td>0.3148592</td>\n",
              "<td>0.4640400</td>\n",
              "<td>0.4631381</td>\n",
              "<td>0.0849377</td>\n",
              "<td>0.8403171</td>\n",
              "<td>-14.8760207</td>\n",
              "<td>20.0828377</td>\n",
              "<td>0.2290475</td></tr>\n",
              "<tr><td>14</td>\n",
              "<td>0.8</td>\n",
              "<td>0.2526527</td>\n",
              "<td>0.6328169</td>\n",
              "<td>1.1296716</td>\n",
              "<td>0.2445415</td>\n",
              "<td>0.2753320</td>\n",
              "<td>0.4365427</td>\n",
              "<td>0.4396110</td>\n",
              "<td>0.0634202</td>\n",
              "<td>0.9037373</td>\n",
              "<td>-36.7183134</td>\n",
              "<td>12.9671574</td>\n",
              "<td>0.1690725</td></tr>\n",
              "<tr><td>15</td>\n",
              "<td>0.8997812</td>\n",
              "<td>0.1830419</td>\n",
              "<td>0.6242425</td>\n",
              "<td>1.0736220</td>\n",
              "<td>0.2412281</td>\n",
              "<td>0.2250453</td>\n",
              "<td>0.4148833</td>\n",
              "<td>0.4158167</td>\n",
              "<td>0.0622877</td>\n",
              "<td>0.9660249</td>\n",
              "<td>-37.5757485</td>\n",
              "<td>7.3622048</td>\n",
              "<td>0.1079650</td></tr>\n",
              "<tr><td>16</td>\n",
              "<td>1.0</td>\n",
              "<td>0.0323629</td>\n",
              "<td>0.3390090</td>\n",
              "<td>1.0</td>\n",
              "<td>0.1310044</td>\n",
              "<td>0.1216746</td>\n",
              "<td>0.3864333</td>\n",
              "<td>0.3863381</td>\n",
              "<td>0.0339751</td>\n",
              "<td>1.0</td>\n",
              "<td>-66.0990965</td>\n",
              "<td>0.0</td>\n",
              "<td>0.0</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "</div></div>\n",
              "<div style='margin: 1em 0 1em 0;'>\n",
              "<style>\n",
              "\n",
              "#h2o-table-9.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-9 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-9 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-9 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table th,\n",
              "#h2o-table-9 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-9 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-9\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption>Cross-Validation Metrics Summary: </caption>\n",
              "    <thead><tr><th></th>\n",
              "<th>mean</th>\n",
              "<th>sd</th>\n",
              "<th>cv_1_valid</th>\n",
              "<th>cv_2_valid</th>\n",
              "<th>cv_3_valid</th>\n",
              "<th>cv_4_valid</th>\n",
              "<th>cv_5_valid</th></tr></thead>\n",
              "    <tbody><tr><td>accuracy</td>\n",
              "<td>0.5800185</td>\n",
              "<td>0.0329346</td>\n",
              "<td>0.6188524</td>\n",
              "<td>0.6037344</td>\n",
              "<td>0.5351474</td>\n",
              "<td>0.578199</td>\n",
              "<td>0.5641593</td></tr>\n",
              "<tr><td>auc</td>\n",
              "<td>0.6994116</td>\n",
              "<td>0.0107985</td>\n",
              "<td>0.712518</td>\n",
              "<td>0.6938667</td>\n",
              "<td>0.6953527</td>\n",
              "<td>0.6866761</td>\n",
              "<td>0.7086443</td></tr>\n",
              "<tr><td>err</td>\n",
              "<td>0.4199815</td>\n",
              "<td>0.0329346</td>\n",
              "<td>0.3811475</td>\n",
              "<td>0.3962656</td>\n",
              "<td>0.4648526</td>\n",
              "<td>0.4218009</td>\n",
              "<td>0.4358407</td></tr>\n",
              "<tr><td>err_count</td>\n",
              "<td>191.4</td>\n",
              "<td>10.310189</td>\n",
              "<td>186.0</td>\n",
              "<td>191.0</td>\n",
              "<td>205.0</td>\n",
              "<td>178.0</td>\n",
              "<td>197.0</td></tr>\n",
              "<tr><td>f0point5</td>\n",
              "<td>0.5218702</td>\n",
              "<td>0.0239401</td>\n",
              "<td>0.5455177</td>\n",
              "<td>0.5408310</td>\n",
              "<td>0.4850498</td>\n",
              "<td>0.5197828</td>\n",
              "<td>0.5181696</td></tr>\n",
              "<tr><td>f1</td>\n",
              "<td>0.6077798</td>\n",
              "<td>0.0149498</td>\n",
              "<td>0.628</td>\n",
              "<td>0.6125761</td>\n",
              "<td>0.5875251</td>\n",
              "<td>0.6008968</td>\n",
              "<td>0.609901</td></tr>\n",
              "<tr><td>f2</td>\n",
              "<td>0.7288278</td>\n",
              "<td>0.0181833</td>\n",
              "<td>0.7398680</td>\n",
              "<td>0.7062675</td>\n",
              "<td>0.7448980</td>\n",
              "<td>0.7120085</td>\n",
              "<td>0.7410972</td></tr>\n",
              "<tr><td>lift_top_group</td>\n",
              "<td>2.4868298</td>\n",
              "<td>0.2404563</td>\n",
              "<td>2.0877006</td>\n",
              "<td>2.5104167</td>\n",
              "<td>2.7391305</td>\n",
              "<td>2.5575757</td>\n",
              "<td>2.539326</td></tr>\n",
              "<tr><td>logloss</td>\n",
              "<td>0.5995073</td>\n",
              "<td>0.0106350</td>\n",
              "<td>0.5975826</td>\n",
              "<td>0.6094921</td>\n",
              "<td>0.5933574</td>\n",
              "<td>0.6109893</td>\n",
              "<td>0.5861149</td></tr>\n",
              "<tr><td>max_per_class_error</td>\n",
              "<td>0.5834913</td>\n",
              "<td>0.0709034</td>\n",
              "<td>0.5182724</td>\n",
              "<td>0.5172414</td>\n",
              "<td>0.6785714</td>\n",
              "<td>0.5719844</td>\n",
              "<td>0.6313869</td></tr>\n",
              "<tr><td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td>\n",
              "<td>---</td></tr>\n",
              "<tr><td>mean_per_class_error</td>\n",
              "<td>0.3707304</td>\n",
              "<td>0.0192374</td>\n",
              "<td>0.3393501</td>\n",
              "<td>0.3653915</td>\n",
              "<td>0.3858696</td>\n",
              "<td>0.3799316</td>\n",
              "<td>0.3831092</td></tr>\n",
              "<tr><td>mse</td>\n",
              "<td>0.2062817</td>\n",
              "<td>0.0042769</td>\n",
              "<td>0.2059348</td>\n",
              "<td>0.2101728</td>\n",
              "<td>0.2029764</td>\n",
              "<td>0.2110075</td>\n",
              "<td>0.2013171</td></tr>\n",
              "<tr><td>null_deviance</td>\n",
              "<td>609.89655</td>\n",
              "<td>38.750076</td>\n",
              "<td>649.6677</td>\n",
              "<td>648.5911</td>\n",
              "<td>580.15045</td>\n",
              "<td>564.853</td>\n",
              "<td>606.22034</td></tr>\n",
              "<tr><td>pr_auc</td>\n",
              "<td>0.6190544</td>\n",
              "<td>0.0304385</td>\n",
              "<td>0.5904093</td>\n",
              "<td>0.6393982</td>\n",
              "<td>0.5958168</td>\n",
              "<td>0.6080447</td>\n",
              "<td>0.6616032</td></tr>\n",
              "<tr><td>precision</td>\n",
              "<td>0.4771197</td>\n",
              "<td>0.0276315</td>\n",
              "<td>0.5015975</td>\n",
              "<td>0.5016611</td>\n",
              "<td>0.4345238</td>\n",
              "<td>0.4768683</td>\n",
              "<td>0.470948</td></tr>\n",
              "<tr><td>r2</td>\n",
              "<td>0.1293294</td>\n",
              "<td>0.0162237</td>\n",
              "<td>0.1287130</td>\n",
              "<td>0.1230570</td>\n",
              "<td>0.1243332</td>\n",
              "<td>0.1138530</td>\n",
              "<td>0.1566906</td></tr>\n",
              "<tr><td>recall</td>\n",
              "<td>0.8420305</td>\n",
              "<td>0.0466999</td>\n",
              "<td>0.8395722</td>\n",
              "<td>0.7864583</td>\n",
              "<td>0.9068323</td>\n",
              "<td>0.8121212</td>\n",
              "<td>0.8651685</td></tr>\n",
              "<tr><td>residual_deviance</td>\n",
              "<td>547.931</td>\n",
              "<td>34.599743</td>\n",
              "<td>583.24066</td>\n",
              "<td>587.55035</td>\n",
              "<td>523.34125</td>\n",
              "<td>515.6749</td>\n",
              "<td>529.8479</td></tr>\n",
              "<tr><td>rmse</td>\n",
              "<td>0.4541630</td>\n",
              "<td>0.0047082</td>\n",
              "<td>0.4538004</td>\n",
              "<td>0.4584461</td>\n",
              "<td>0.4505291</td>\n",
              "<td>0.4593555</td>\n",
              "<td>0.4486837</td></tr>\n",
              "<tr><td>specificity</td>\n",
              "<td>0.4165087</td>\n",
              "<td>0.0709034</td>\n",
              "<td>0.4817276</td>\n",
              "<td>0.4827586</td>\n",
              "<td>0.3214286</td>\n",
              "<td>0.4280156</td>\n",
              "<td>0.3686131</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n",
              "<pre style='font-size: smaller; margin-bottom: 1em;'>[22 rows x 8 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
            ],
            "text/plain": [
              "Model Details\n",
              "=============\n",
              "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
              "Model Key: StackedEnsemble_BestOfFamily_4_AutoML_2_20231025_41020\n",
              "\n",
              "\n",
              "Model Summary for Stacked Ensemble: \n",
              "key                                        value\n",
              "-----------------------------------------  ----------------\n",
              "Stacking strategy                          cross_validation\n",
              "Number of base models (used / total)       4/6\n",
              "# GBM base models (used / total)           1/1\n",
              "# XGBoost base models (used / total)       0/1\n",
              "# DeepLearning base models (used / total)  1/1\n",
              "# DRF base models (used / total)           2/2\n",
              "# GLM base models (used / total)           0/1\n",
              "Metalearner algorithm                      GLM\n",
              "Metalearner fold assignment scheme         Random\n",
              "Metalearner nfolds                         5\n",
              "Metalearner fold_column\n",
              "Custom metalearner hyperparameters         None\n",
              "\n",
              "ModelMetricsBinomialGLM: stackedensemble\n",
              "** Reported on train data. **\n",
              "\n",
              "MSE: 0.1031204083854706\n",
              "RMSE: 0.32112366525292185\n",
              "LogLoss: 0.3572143407453567\n",
              "AUC: 0.9768681046167664\n",
              "AUCPR: 0.9550685580539391\n",
              "Gini: 0.9537362092335329\n",
              "Null degrees of freedom: 2284\n",
              "Residual degrees of freedom: 2280\n",
              "Null deviance: 3048.7652424086486\n",
              "Residual deviance: 1632.46953720628\n",
              "AIC: 1642.46953720628\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.39520420066275386\n",
              "       0     1    Error    Rate\n",
              "-----  ----  ---  -------  --------------\n",
              "0      1301  101  0.072    (101.0/1402.0)\n",
              "1      42    841  0.0476   (42.0/883.0)\n",
              "Total  1343  942  0.0626   (143.0/2285.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.395204     0.921644  207\n",
              "max f2                       0.357116     0.947972  221\n",
              "max f0point5                 0.466401     0.925881  179\n",
              "max accuracy                 0.428456     0.938293  195\n",
              "max precision                0.999519     1         0\n",
              "max recall                   0.160634     1         333\n",
              "max specificity              0.999519     1         0\n",
              "max absolute_mcc             0.395204     0.8709    207\n",
              "max min_per_class_accuracy   0.415062     0.935806  200\n",
              "max mean_per_class_accuracy  0.375565     0.941077  213\n",
              "max tns                      0.999519     1402      0\n",
              "max fns                      0.999519     864       0\n",
              "max fps                      0.00108754   1402      399\n",
              "max tps                      0.160634     883       333\n",
              "max tnr                      0.999519     1         0\n",
              "max fnr                      0.999519     0.978482  0\n",
              "max fpr                      0.00108754   1         399\n",
              "max tpr                      0.160634     1         333\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 38.64 %, avg score: 39.46 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.0100656                   0.996874           2.58777    2.58777            1                0.999178   1                           0.999178            0.0260476       0.0260476                  158.777   158.777            0.0260476\n",
              "2        0.0201313                   0.982329           2.47526    2.53151            0.956522         0.990753   0.978261                    0.994966            0.0249151       0.0509626                  147.526   153.151            0.0502494\n",
              "3        0.0301969                   0.957896           2.58777    2.55027            1                0.970809   0.985507                    0.986913            0.0260476       0.0770102                  158.777   155.027            0.0762969\n",
              "4        0.0402626                   0.936903           2.58777    2.55964            1                0.947945   0.98913                     0.977171            0.0260476       0.103058                   158.777   155.964            0.102344\n",
              "5        0.0503282                   0.902161           2.58777    2.56527            1                0.919684   0.991304                    0.965674            0.0260476       0.129105                   158.777   156.527            0.128392\n",
              "6        0.100219                    0.780511           2.51967    2.54257            0.973684         0.84351    0.982533                    0.904858            0.125708        0.254813                   151.967   154.257            0.25196\n",
              "7        0.150109                    0.683712           2.36077    2.48215            0.912281         0.731785   0.959184                    0.847336            0.11778         0.372593                   136.077   148.215            0.362608\n",
              "8        0.2                         0.611948           2.51967    2.49151            0.973684         0.647065   0.962801                    0.797378            0.125708        0.498301                   151.967   149.151            0.486176\n",
              "9        0.300219                    0.512231           2.44087    2.4746             0.943231         0.560409   0.956268                    0.718273            0.244621        0.742922                   144.087   147.46             0.721524\n",
              "10       0.4                         0.415284           1.92948    2.33862            0.745614         0.468695   0.90372                     0.656015            0.192525        0.935447                   92.9477   133.862            0.87268\n",
              "11       0.500219                    0.31141            0.519814   1.97422            0.200873         0.349688   0.762905                    0.594642            0.0520951       0.987542                   -48.0186  97.4221            0.794247\n",
              "12       0.6                         0.26907            0.079449   1.65912            0.0307018        0.289416   0.641138                    0.543883            0.00792752      0.99547                    -92.0551  65.9117            0.644543\n",
              "13       0.699781                    0.230599           0.0226997  1.42578            0.00877193       0.24929    0.550969                    0.501877            0.00226501      0.997735                   -97.73    42.5781            0.485609\n",
              "14       0.8                         0.18788            0.0113003  1.24858            0.00436681       0.209727   0.482495                    0.465278            0.0011325       0.998867                   -98.87    24.8584            0.324117\n",
              "15       0.899781                    0.120593           0.0113499  1.11138            0.00438596       0.158645   0.429475                    0.431274            0.0011325       1                          -98.865   11.1381            0.163338\n",
              "16       1                           0.000314511        0          1                  0                0.0651503  0.386433                    0.394582            0               1                          -100      0                  0\n",
              "\n",
              "ModelMetricsBinomialGLM: stackedensemble\n",
              "** Reported on cross-validation data. **\n",
              "\n",
              "MSE: 0.2062812126266294\n",
              "RMSE: 0.4541819157855467\n",
              "LogLoss: 0.5994868909822116\n",
              "AUC: 0.7002046098196557\n",
              "AUCPR: 0.6174190344601261\n",
              "Gini: 0.4004092196393114\n",
              "Null degrees of freedom: 2284\n",
              "Residual degrees of freedom: 2280\n",
              "Null deviance: 3049.4826908427112\n",
              "Residual deviance: 2739.655091788707\n",
              "AIC: 2749.655091788707\n",
              "\n",
              "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.30834730182737674\n",
              "       0    1     Error    Rate\n",
              "-----  ---  ----  -------  --------------\n",
              "0      591  811   0.5785   (811.0/1402.0)\n",
              "1      154  729   0.1744   (154.0/883.0)\n",
              "Total  745  1540  0.4223   (965.0/2285.0)\n",
              "\n",
              "Maximum Metrics: Maximum metrics at their respective thresholds\n",
              "metric                       threshold    value     idx\n",
              "---------------------------  -----------  --------  -----\n",
              "max f1                       0.308347     0.601733  270\n",
              "max f2                       0.168922     0.763827  351\n",
              "max f0point5                 0.470778     0.578499  154\n",
              "max accuracy                 0.470778     0.685339  154\n",
              "max precision                0.999393     1         0\n",
              "max recall                   0.058658     1         394\n",
              "max specificity              0.999393     1         0\n",
              "max absolute_mcc             0.470778     0.303317  154\n",
              "max min_per_class_accuracy   0.371629     0.637599  223\n",
              "max mean_per_class_accuracy  0.372849     0.640567  222\n",
              "max tns                      0.999393     1402      0\n",
              "max fns                      0.999393     882       0\n",
              "max fps                      0.0336666    1402      399\n",
              "max tps                      0.058658     883       394\n",
              "max tnr                      0.999393     1         0\n",
              "max fnr                      0.999393     0.998867  0\n",
              "max fpr                      0.0336666    1         399\n",
              "max tpr                      0.058658     1         394\n",
              "\n",
              "Gains/Lift Table: Avg response rate: 38.64 %, avg score: 38.63 %\n",
              "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
              "-------  --------------------------  -----------------  --------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
              "1        0.0100656                   0.896796           2.47526   2.47526            0.956522         0.941905  0.956522                    0.941905            0.0249151       0.0249151                  147.526   147.526            0.0242018\n",
              "2        0.0201313                   0.846049           2.25023   2.36275            0.869565         0.87086   0.913043                    0.906382            0.0226501       0.0475651                  125.023   136.275            0.0447121\n",
              "3        0.0301969                   0.813541           1.80019   2.17523            0.695652         0.830491  0.84058                     0.881085            0.01812         0.0656852                  80.0187   117.523            0.0578392\n",
              "4        0.0402626                   0.773752           2.02521   2.13772            0.782609         0.797252  0.826087                    0.860127            0.0203851       0.0860702                  102.521   113.772            0.0746579\n",
              "5        0.0503282                   0.74277            2.25023   2.16022            0.869565         0.7587    0.834783                    0.839842            0.0226501       0.10872                    125.023   116.022            0.0951682\n",
              "6        0.100219                    0.634772           1.92948   2.04535            0.745614         0.688276  0.790393                    0.76439             0.0962627       0.204983                   92.9477   104.535            0.170746\n",
              "7        0.150109                    0.553839           1.40738   1.83332            0.54386          0.589901  0.708455                    0.706396            0.0702152       0.275198                   40.7383   83.3317            0.203872\n",
              "8        0.2                         0.496671           1.40738   1.72707            0.54386          0.522318  0.667396                    0.660477            0.0702152       0.345413                   40.7383   72.7067            0.236997\n",
              "9        0.300219                    0.437443           1.13003   1.52776            0.436681         0.465868  0.590379                    0.595513            0.11325         0.458664                   13.003    52.7764            0.258236\n",
              "10       0.4                         0.395477           1.12364   1.42695            0.434211         0.415417  0.551422                    0.550588            0.112118        0.570781                   12.3637   42.6954            0.278342\n",
              "11       0.500219                    0.362659           0.949225  1.33124            0.366812         0.377035  0.514436                    0.515816            0.0951302       0.665912                   -5.07747  33.1241            0.270049\n",
              "12       0.6                         0.331313           0.896639  1.25897            0.346491         0.347333  0.486506                    0.487797            0.0894677       0.755379                   -10.3361  25.8966            0.25324\n",
              "13       0.699781                    0.297634           0.85124   1.20083            0.328947         0.314859  0.46404                     0.463138            0.0849377       0.840317                   -14.876   20.0828            0.229047\n",
              "14       0.8                         0.252653           0.632817  1.12967            0.244541         0.275332  0.436543                    0.439611            0.0634202       0.903737                   -36.7183  12.9672            0.169072\n",
              "15       0.899781                    0.183042           0.624243  1.07362            0.241228         0.225045  0.414883                    0.415817            0.0622877       0.966025                   -37.5757  7.3622             0.107965\n",
              "16       1                           0.0323629          0.339009  1                  0.131004         0.121675  0.386433                    0.386338            0.0339751       1                          -66.0991  0                  0\n",
              "\n",
              "Cross-Validation Metrics Summary: \n",
              "                      mean        sd            cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
              "--------------------  ----------  ------------  ------------  ------------  ------------  ------------  ------------\n",
              "accuracy              0.5800185   0.032934643   0.61885244    0.60373443    0.53514737    0.578199      0.5641593\n",
              "auc                   0.6994116   0.010798533   0.712518      0.6938667     0.69535273    0.6866761     0.70864433\n",
              "err                   0.41998148  0.032934643   0.38114753    0.39626557    0.4648526     0.42180094    0.4358407\n",
              "err_count             191.4       10.310189     186.0         191.0         205.0         178.0         197.0\n",
              "f0point5              0.5218702   0.023940062   0.54551774    0.54083097    0.48504984    0.5197828     0.5181696\n",
              "f1                    0.6077798   0.014949798   0.628         0.61257607    0.5875251     0.60089684    0.609901\n",
              "f2                    0.72882783  0.0181833     0.73986804    0.70626754    0.74489796    0.7120085     0.7410972\n",
              "lift_top_group        2.4868298   0.24045631    2.0877006     2.5104167     2.7391305     2.5575757     2.539326\n",
              "logloss               0.5995073   0.010634983   0.59758264    0.60949206    0.59335744    0.6109893     0.58611494\n",
              "max_per_class_error   0.5834913   0.07090341    0.5182724     0.51724136    0.6785714     0.5719844     0.6313869\n",
              "---                   ---         ---           ---           ---           ---           ---           ---\n",
              "mean_per_class_error  0.3707304   0.019237438   0.3393501     0.36539152    0.38586956    0.3799316     0.38310915\n",
              "mse                   0.20628172  0.0042768745  0.20593478    0.21017282    0.20297644    0.21100752    0.20131709\n",
              "null_deviance         609.89655   38.750076     649.6677      648.5911      580.15045     564.853       606.22034\n",
              "pr_auc                0.61905444  0.03043851    0.5904093     0.6393982     0.59581685    0.6080447     0.66160315\n",
              "precision             0.47711974  0.027631525   0.50159746    0.5016611     0.43452382    0.47686833    0.470948\n",
              "r2                    0.12932935  0.016223738   0.12871298    0.123056956   0.124333195   0.11385303    0.1566906\n",
              "recall                0.8420305   0.04669988    0.8395722     0.7864583     0.9068323     0.8121212     0.8651685\n",
              "residual_deviance     547.931     34.599743     583.24066     587.55035     523.34125     515.6749      529.8479\n",
              "rmse                  0.45416296  0.0047081523  0.45380038    0.4584461     0.45052907    0.45935553    0.44868374\n",
              "specificity           0.4165087   0.07090341    0.48172757    0.4827586     0.32142857    0.42801556    0.36861312\n",
              "[22 rows x 8 columns]\n",
              "\n",
              "\n",
              "[tips]\n",
              "Use `model.explain()` to inspect the model.\n",
              "--\n",
              "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aml = H2OAutoML(balance_classes=True)\n",
        "aml.train(x=X, y=y, training_frame=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "d4sqsWjpcX64",
        "outputId": "dbfe67b3-6ba0-40c2-ccb4-4260e71116a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "H2OFrame is empty."
            ],
            "text/plain": [
              "H2OFrame is empty."
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aml.leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C8ztuwJceAQ"
      },
      "outputs": [],
      "source": [
        "type(test['Potability'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdBJe61ycezO"
      },
      "outputs": [],
      "source": [
        "pred = aml.leader.predict(test)\n",
        "y_val = h2o.as_list(test['Potability'], use_pandas=True)\n",
        "pred_val = h2o.as_list(pred['predict'], use_pandas=True)\n",
        "print(classification_report(y_val,pred_val))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}